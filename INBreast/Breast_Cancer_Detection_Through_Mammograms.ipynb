{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10529195,"sourceType":"datasetVersion","datasetId":6516012}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast Cancer Detection With Mammograms\n\nThis is a demonstration on how to build a model that gives prediction on breast cancer given a set of mammograms.\n\nThe dataset used is the [Augmented INBreast Dataset](https://www.kaggle.com/datasets/eoussama/breast-cancer-mammograms).\n\nA mirror repository for this notebook can be found on [github](https://github.com/gomu-gomu/ma-dl-projet-1).\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow==2.12.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:25:31.752493Z","iopub.execute_input":"2025-07-02T10:25:31.752835Z","iopub.status.idle":"2025-07-02T10:26:56.770448Z","shell.execute_reply.started":"2025-07-02T10:25:31.752802Z","shell.execute_reply":"2025-07-02T10:26:56.769324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.callbacks import CSVLogger\nfrom keras.models import Sequential, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:26:56.771403Z","iopub.execute_input":"2025-07-02T10:26:56.771705Z","iopub.status.idle":"2025-07-02T10:27:04.132895Z","shell.execute_reply.started":"2025-07-02T10:26:56.771678Z","shell.execute_reply":"2025-07-02T10:27:04.132083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset INBreast Dataset\nWe have to prepare our dataset and split it in a convenient way. First we start by setting up separate folders fore each sub-category.","metadata":{}},{"cell_type":"code","source":"output_path = \"/kaggle/working\"\ninput_path = \"/kaggle/input/breast-cancer-mammograms/\"\ndata_path = f\"{output_path}/split_data\"\n\nfor split in ['train', 'val', 'test']:\n    for cls in ['benign', 'malignant']:\n        os.makedirs(os.path.join(data_path, split, cls), exist_ok=True)\n\ndef get_files(cls):\n    return [f for f in os.listdir(os.path.join(input_path, cls)) \n           if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\nbenign_files = get_files('benign')\nmalignant_files = get_files('malignant')\n\ndef split_data(files):\n    train, temp = train_test_split(files, test_size=0.2, random_state=42)\n    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n    return train, val, test\n\nbenign_train, benign_val, benign_test = split_data(benign_files)\nmalignant_train, malignant_val, malignant_test = split_data(malignant_files)\n\ndef copy_files(files, cls, split):\n    for f in files:\n        src = os.path.join(input_path, cls, f)\n        dst = os.path.join(data_path, split, cls, f)\n        shutil.copy(src, dst)\n\ncopy_files(benign_train, 'benign', 'train')\ncopy_files(benign_val, 'benign', 'val')\ncopy_files(benign_test, 'benign', 'test')\n\ncopy_files(malignant_train, 'malignant', 'train')\ncopy_files(malignant_val, 'malignant', 'val')\ncopy_files(malignant_test, 'malignant', 'test')\n\nprint(\"Dataset split completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:27:04.133842Z","iopub.execute_input":"2025-07-02T10:27:04.134617Z","iopub.status.idle":"2025-07-02T10:27:40.608328Z","shell.execute_reply.started":"2025-07-02T10:27:04.134578Z","shell.execute_reply":"2025-07-02T10:27:40.607404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading the data\nThen we create an image data generator that will scale each pixel's value by 1/255, essentially normalizing them from the standard 0–255 range to a 0–1 range. Then we load the three data collections separately.","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1./255)\n\ntrain = datagen.flow_from_directory(\n    f'{data_path}/train',\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=64\n)\n\nval = datagen.flow_from_directory(\n    f'{data_path}/val',\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=64\n)\n\ntest = datagen.flow_from_directory(\n    f'{data_path}/test',\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=64\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:27:40.610165Z","iopub.execute_input":"2025-07-02T10:27:40.610478Z","iopub.status.idle":"2025-07-02T10:27:40.752834Z","shell.execute_reply.started":"2025-07-02T10:27:40.610453Z","shell.execute_reply":"2025-07-02T10:27:40.751832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data inspection\nNow we fetch the first batch of images and labels out of `train`, and confirm their shape.","metadata":{}},{"cell_type":"code","source":"imgs, labels = next(train)\nimgs.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:27:40.754199Z","iopub.execute_input":"2025-07-02T10:27:40.754570Z","iopub.status.idle":"2025-07-02T10:27:40.903242Z","shell.execute_reply.started":"2025-07-02T10:27:40.754535Z","shell.execute_reply":"2025-07-02T10:27:40.902292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **64**: the batch size (number of images).\n* **224 × 224**: the height and width of each image, as defined in `target_size=(224, 224)`.\n* **3**: the number of color channels (RGB).\n\nWe confirm that we only have 2 classes. **Benign** and **Malgnant**.","metadata":{}},{"cell_type":"code","source":"train.class_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:27:40.904219Z","iopub.execute_input":"2025-07-02T10:27:40.904495Z","iopub.status.idle":"2025-07-02T10:27:40.910109Z","shell.execute_reply.started":"2025-07-02T10:27:40.904472Z","shell.execute_reply":"2025-07-02T10:27:40.909103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Displaying the first image in the training dataset should show us a grayscale scan of a breast, accompanied with the correct class decided by its diagnosis.","metadata":{}},{"cell_type":"code","source":"plt.imshow(imgs[0])\nprint(f'Class: {labels[0]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:27:40.911493Z","iopub.execute_input":"2025-07-02T10:27:40.911910Z","iopub.status.idle":"2025-07-02T10:27:41.209398Z","shell.execute_reply.started":"2025-07-02T10:27:40.911873Z","shell.execute_reply":"2025-07-02T10:27:41.208436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The class is `1.0`, which matches **Malignant** on the class indices of the training dataset.","metadata":{}},{"cell_type":"markdown","source":"## Model\n\nWe're gonna build a simple Convolutional Neural Network (CNN) for binary classification. First, we add three convolution-and-pooling blocks to extract spatial features from the images. Then, we flatten the feature maps and pass them through a couple of dense (fully connected) layers, including a dropout for regularization. Finally, we use a single output neuron with a sigmoid activation for predicting whether an image is benign or malignant.\n\n* Step 1:\n\n    - We start by initializing a new `Sequential` model, which is a linear stack of layers in Keras.\n* Step 2:\n\n    - `Conv2D(32, (3, 3))`: A convolutional layer with 32 filters/kernels, each 3×3 in size.\n    - `input_shape=(224,224,3)`: This is the expected shape of the input images. 224×224 resolution, 3 color channels (RGB).\n    - `activation='relu'`: Uses the ReLU activation function.\n    - `MaxPooling2D(pool_size=(2, 2))`: Reduces the spatial dimensions by taking the maximum value in each 2×2 pool region, effectively halving the height and width.\n* Step 3:\n\n    - Similar to the first block, but it doesn't need an input_shape since Keras automatically infers the shape from the previous layer's output.\n    - Another 32 filters, each 3×3, followed by a 2×2 max pool to further downsample.\n* Step 4:\n\n    - Now we increase the number of filters to 64. A larger number of filters can learn more complex features.\n    - Followed by another 2×2 max pool for further spatial reduction.\n* Step 5:\n\n    - `Flatten()`: Converts the 3D feature maps (height × width × channels) into a 1D vector, so it can be fed into a dense (fully connected) layer.\n    - `Dense(64, activation='relu')`: A fully connected layer with 64 neurons, learning high-level combinations of features.\n    - `Dropout(0.5)`: Randomly sets 50% of the neurons to zero during training to reduce overfitting.\n    - `Dense(1, activation='sigmoid')`: The final output layer. Since this is a binary classification (benign vs. malignant), we use a single neuron with a sigmoid activation, which outputs a probability between 0 and 1.\n* Step 6:\n\n    - `loss='binary_crossentropy'`: The appropriate loss function for binary classification.\n    - `optimizer='rmsprop'`: A gradient-based optimization algorithm (alternative to Adam, SGD, etc.).\n    - `metrics=['accuracy']`: We track accuracy during training/evaluation.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.metrics import AUC, Precision, Recall\n\n\n# Step 1 - Initializing model\nmodel = Sequential()\n\n# Step 2 - First Convolution + MaxPooling\nmodel.add(Conv2D(32, (3, 3), input_shape=(224,224,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Step 3 - Second Convolution + MaxPooling\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Step 4 - Third Convolution + MaxPooling\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Step 5 - Flatten + Dense + Dropout + Output\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Step 6 - Compile\n# Step 6 - Compile\n\n\n# additional metrics\nmodel.compile(loss='binary_crossentropy', \n              optimizer='rmsprop', \n              metrics=['accuracy', AUC(name='auc'), Precision(name='precision'), Recall(name='recall')])\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:31:22.027918Z","iopub.execute_input":"2025-07-02T10:31:22.029222Z","iopub.status.idle":"2025-07-02T10:31:22.208213Z","shell.execute_reply.started":"2025-07-02T10:31:22.029155Z","shell.execute_reply":"2025-07-02T10:31:22.207236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Output Shape**: The size of the activation maps after each layer.\n\n- **Param #**: The number of trainable parameters (weights and biases).\n\n> Notice that most of the parameters (over 2.7 million) are in the fully connected layer (dense_6), which converts the flattened feature maps to 64 neurons.\n\n- **Total params**: The sum of all parameters in the network.","metadata":{}},{"cell_type":"markdown","source":"## Training\n\nIn this step we will train the CNN for 100 epochs while logging metrics to a CSV file (training.log) for analysis, and then saves the trained model for future use.","metadata":{}},{"cell_type":"code","source":"\n\n# Calculating how many batches (steps) makeup one full pass (epoch) through our training and validation datasets.\nSTEP_SIZE_TRAIN = train.n // train.batch_size\nSTEP_SIZE_VAL = val.n // val.batch_size\n\nos.makedirs(os.path.join(output_path, 'logs'), exist_ok=True)\nos.makedirs(os.path.join(output_path, 'out'), exist_ok=True)\n\n# This callback records loss, accuracy, and other metrics for each epoch into a CSV file named training.log.\n# It's helpful for tracking and comparing training progress over time.\ncsv_logger = CSVLogger(f'{output_path}/logs/training.log', separator=',', append=False)\n\nhistory = model.fit_generator(generator = train, \n                    steps_per_epoch=STEP_SIZE_TRAIN, \n                    validation_data=test,\n                    validation_steps=STEP_SIZE_VAL,\n                    epochs = 50,\n                    # epochs=100,\n                    callbacks=[csv_logger])\n\n\n\n\n\n\n\n\n\n\nmodel.save(f'{output_path}/out/model.h5')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:31:40.667408Z","iopub.execute_input":"2025-07-02T10:31:40.667786Z","iopub.status.idle":"2025-07-02T14:07:02.256095Z","shell.execute_reply.started":"2025-07-02T10:31:40.667758Z","shell.execute_reply":"2025-07-02T14:07:02.252882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To determine the step sizes, take note of the following:\n\n\n\n\n\n\n- `train.n` and `val.n`: The total number of samples in each dataset.\n- `train.batch_size` and `val.batch_size`: The batch size.\n- Using integer division `//` gives you the number of batches needed.\n\nAs for fitting the model:\n- `model.fit_generator(...)`: Trains the model using the data generated from train (the training image generator) in batches.\n- `steps_per_epoch=STEP_SIZE_TRAIN`: How many steps (batches) to run per epoch during training.\n- `validation_data=test` and `validation_steps=STEP_SIZE_VAL`: Here, you're using the test generator for validation, with STEP_SIZE_VAL batches per epoch.\n- `epochs=100`: The number of times the model will see the entire dataset.\n- `callbacks=[csv_logger]`: Logs each epoch's metrics to the CSV file.\n\nWe can visualize how our model's accuracy changes over the course of training. By plotting both the training and validation accuracy, we'll get a clear picture of how well the model is fitting to the training data and how effectively it generalizes to unseen data.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\n\nplt.plot(np.arange(1, len(history.history['accuracy'])+1,1), history.history['accuracy'], color='navy', label = 'Accuracy')\nplt.plot(np.arange(1, len(history.history['accuracy'])+1,1), history.history['val_accuracy'], color='red', label='Validation Accuracy')\nplt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:07:09.969317Z","iopub.execute_input":"2025-07-02T14:07:09.969810Z","iopub.status.idle":"2025-07-02T14:07:10.432359Z","shell.execute_reply.started":"2025-07-02T14:07:09.969750Z","shell.execute_reply":"2025-07-02T14:07:10.431089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot shows two lines: one for the training **accuracy** (in navy) and another for the validation accuracy (in red). As the number of epochs increases, we can observe whether the model converges, if it overfits (training accuracy outpacing validation accuracy), or if both accuracies improve steadily over time. This helps us assess the model's performance and decide on further tuning.\n\nFrom the plot, we can see that the training accuracy (blue line) reaches near-perfect levels, while the validation accuracy (red line) plateaus around the high 80s to low 90s. This suggests the model is learning effectively but also overfitting somewhat—its performance on unseen data (validation accuracy) isn't as high as on the training set. Even so, the validation accuracy still remains robust, indicating that the model generally performs well at classifying new examples despite not matching the near-perfect training performance.\n\nWe can do the same with the saved model. We can read back the CSV file (training.log) that was generated by the `CSVLogger` during training. Each row in this file corresponds to an epoch, and the columns contain metrics like loss, accuracy, validation loss, and validation accuracy.","metadata":{}},{"cell_type":"code","source":"log_data = pd.read_csv(f'{output_path}/logs/training.log', sep=',', engine='python')\nlog_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:07:17.815801Z","iopub.execute_input":"2025-07-02T14:07:17.816145Z","iopub.status.idle":"2025-07-02T14:07:17.908805Z","shell.execute_reply.started":"2025-07-02T14:07:17.816118Z","shell.execute_reply":"2025-07-02T14:07:17.907637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\n\nplt.plot(np.arange(1, len(log_data['accuracy'])+1,1), log_data['accuracy'], color='navy', label = 'Accuracy')\nplt.plot(np.arange(1, len(log_data['accuracy'])+1,1), log_data['val_accuracy'], color='red', label='Validation Accuracy')\nplt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:07:24.883282Z","iopub.execute_input":"2025-07-02T14:07:24.883713Z","iopub.status.idle":"2025-07-02T14:07:25.243999Z","shell.execute_reply.started":"2025-07-02T14:07:24.883684Z","shell.execute_reply":"2025-07-02T14:07:25.242848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Again, visualizing how our model's **training loss** (in navy) compares to the **validation loss** (in red) over each epoch of training. By charting both curves, we can spot whether the model is successfully generalizing (both losses decreasing together) or if it begins to overfit (training loss keeps going down while validation loss rises). This step is essential to ensure our model is learning appropriately and to guide further tuning.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\n\nplt.plot(np.arange(1, len(history.history['loss'])+1,1), history.history['loss'], color='navy', label = 'Loss')\nplt.plot(np.arange(1, len(history.history['loss'])+1,1), history.history['val_loss'], color='red', label='Validation Loss')\nplt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:07:30.088614Z","iopub.execute_input":"2025-07-02T14:07:30.089019Z","iopub.status.idle":"2025-07-02T14:07:30.430233Z","shell.execute_reply.started":"2025-07-02T14:07:30.088970Z","shell.execute_reply":"2025-07-02T14:07:30.429230Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From this plot, it's clear that the **training loss** (blue) steadily goes down to very low values, indicating the model is fitting the training data extremely well. However, the **validation loss** (red) varies up and down rather than following the same steady downward trend, suggesting the model is **overfitting** and not generalizing as consistently to unseen data. The spikes in the validation loss can occur for various reasons (e.g., data variability, small validation sets, or model instability), but the key takeaway is that while the model memorizes the training set effectively, its performance on new data is less stable.\n\nDoing the same again with the logged data.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\n\nplt.plot(np.arange(1, len(log_data['loss'])+1,1), log_data['loss'], color='navy', label = 'Loss')\nplt.plot(np.arange(1, len(log_data['loss'])+1,1), log_data['val_loss'], color='red', label='Validation Loss')\nplt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:07:34.797626Z","iopub.execute_input":"2025-07-02T14:07:34.797999Z","iopub.status.idle":"2025-07-02T14:07:35.145115Z","shell.execute_reply.started":"2025-07-02T14:07:34.797969Z","shell.execute_reply":"2025-07-02T14:07:35.143984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print classification report before hibernation \nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\n# Step 1: Load the trained model\nfrom tensorflow.keras.models import load_model\nmodel = load_model(f'{output_path}/out/model.h5')\n\n# Step 2: Prepare the test data generator\ntest_generator = test  # Your existing test data generator\n\n# Step 3: Predict the probabilities on the test set\n# Note: Use predict() with batch size as needed\npred_probs = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n# For binary classification, convert probabilities to class labels\npred_labels = (pred_probs > 0.5).astype(int).flatten()\n\n# Step 4: Get true labels\ntrue_labels = test_generator.classes\n\n# Step 5: Generate classification report\nreport = classification_report(true_labels, pred_labels, target_names=['benign', 'malignant'])\nprint(\"Classification Report:\\n\", report)\n\n# Optional: Confusion matrix\ncm = confusion_matrix(true_labels, pred_labels)\nprint(\"Confusion Matrix:\\n\", cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:07:39.607024Z","iopub.execute_input":"2025-07-02T14:07:39.607407Z","iopub.status.idle":"2025-07-02T14:07:49.773215Z","shell.execute_reply.started":"2025-07-02T14:07:39.607377Z","shell.execute_reply":"2025-07-02T14:07:49.771942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Plotting the confusion matrix\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:08:09.759807Z","iopub.execute_input":"2025-07-02T14:08:09.760189Z","iopub.status.idle":"2025-07-02T14:08:10.032752Z","shell.execute_reply.started":"2025-07-02T14:08:09.760138Z","shell.execute_reply":"2025-07-02T14:08:10.031689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Normalize the confusion matrix to percentages\ncm_percentage = (cm.astype('float') / cm.sum()) * 100\n\n# Plotting the confusion matrix as percentages\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix in Percentages')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:08:15.304303Z","iopub.execute_input":"2025-07-02T14:08:15.304700Z","iopub.status.idle":"2025-07-02T14:08:15.567045Z","shell.execute_reply.started":"2025-07-02T14:08:15.304664Z","shell.execute_reply":"2025-07-02T14:08:15.566094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:11:54.891699Z","iopub.execute_input":"2025-07-02T14:11:54.892128Z","iopub.status.idle":"2025-07-02T14:11:54.928064Z","shell.execute_reply.started":"2025-07-02T14:11:54.892098Z","shell.execute_reply":"2025-07-02T14:11:54.926534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot AUC\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['auc'], label='Train AUC')\nplt.plot(history.history['val_auc'], label='Val AUC')\nplt.title('AUC over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('AUC')\nplt.legend()\nplt.grid(True)\nplt.savefig(\"AUC_Epochs.png\", dpi=300)\nplt.show()\n\n# Plot Precision\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['precision'], label='Train Precision')\nplt.plot(history.history['val_precision'], label='Val Precision')\nplt.title('Precision over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid(True)\nplt.savefig(\"Precision_Epochs.png\", dpi=300)\nplt.show()\n\n# Plot Recall\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['recall'], label='Train Recall')\nplt.plot(history.history['val_recall'], label='Val Recall')\nplt.title('Recall over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Recall')\nplt.legend()\nplt.grid(True)\nplt.savefig(\"Recall_Epochs.png\", dpi=300)\nplt.show()\n\n# Plot Accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.savefig(\"Accuracy_Epochs.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:09:05.783520Z","iopub.execute_input":"2025-07-02T14:09:05.783921Z","iopub.status.idle":"2025-07-02T14:09:09.022298Z","shell.execute_reply.started":"2025-07-02T14:09:05.783891Z","shell.execute_reply":"2025-07-02T14:09:09.021121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import (\n    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Predict on all validation data\ny_pred_prob = model.predict(test, verbose=1)\n\n# Class prediction (binary threshold)\ny_pred_class = (y_pred_prob > 0.5).astype(int).flatten()\n\n# Ground truth labels\ny_true = test.classes  # length = test.n\n\n# Ensure alignment (optional but safe)\nassert len(y_true) == len(y_pred_class)\n\n# Metrics\nprecision = precision_score(y_true, y_pred_class)\nrecall = recall_score(y_true, y_pred_class)\nf1 = f1_score(y_true, y_pred_class)\nauc_score = roc_auc_score(y_true, y_pred_prob)\n\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall:    {recall:.4f}\")\nprint(f\"Test F1 Score:  {f1:.4f}\")\nprint(f\"Test AUC:       {auc_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:12:14.399775Z","iopub.execute_input":"2025-07-02T14:12:14.400115Z","iopub.status.idle":"2025-07-02T14:12:24.527314Z","shell.execute_reply.started":"2025-07-02T14:12:14.400082Z","shell.execute_reply":"2025-07-02T14:12:24.526310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:12:44.461114Z","iopub.execute_input":"2025-07-02T14:12:44.461534Z","iopub.status.idle":"2025-07-02T14:12:44.772718Z","shell.execute_reply.started":"2025-07-02T14:12:44.461502Z","shell.execute_reply":"2025-07-02T14:12:44.771680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hibernation\n\nWe can load our previously saved model from disk, then extract all the images and labels from our test generator into NumPy arrays. This prepares our entire test set in a convenient format for further evaluation or predictions.","metadata":{}},{"cell_type":"code","source":"# Loading the previously saved model from the specified path. This includes the architecture, weights, and training configuration (if any).\nrestored_model = load_model(f'{output_path}/out/model.h5')\n\n# Calculating how many batches you need to go through in order to cover our entire test set\nsteps = test.n//test.batch_size\n\n# Reseting the test generator to start yielding batches from the first image again. This is useful if the generator's internal index was advanced by previous calls (e.g., during validation).\ntest.reset()\n\n# Collecting the test data\nX_test, y_test = [], []\nfor i in range(steps):\n    a , b = test.next()\n    X_test.extend(a) \n    y_test.extend(b)\n\n# Converting lists to NumPy arrays\nX_test, y_test = np.array(X_test), np.array(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:13:27.794269Z","iopub.execute_input":"2025-07-02T14:13:27.794668Z","iopub.status.idle":"2025-07-02T14:13:29.621760Z","shell.execute_reply.started":"2025-07-02T14:13:27.794638Z","shell.execute_reply":"2025-07-02T14:13:29.620826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can save our test data arrays (X_test and y_test) to disk using `pickle.dump` and then reload them with `pickle.load`.","metadata":{}},{"cell_type":"code","source":"# Pickling the model\npickle.dump(X_test, open(f'{output_path}/out/X_test.pkl', 'wb'))\npickle.dump(y_test, open(f'{output_path}/out/y_test.pkl', 'wb'))\n\n# Reloading the model to ensure not thing is corrupt\nX_test = pickle.load(open(f'{output_path}/out/X_test.pkl', 'rb'))\ny_test = pickle.load(open(f'{output_path}/out/y_test.pkl', 'rb'))\n\nprint(X_test.shape, y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:13:33.292635Z","iopub.execute_input":"2025-07-02T14:13:33.292992Z","iopub.status.idle":"2025-07-02T14:13:34.459154Z","shell.execute_reply.started":"2025-07-02T14:13:33.292963Z","shell.execute_reply":"2025-07-02T14:13:34.458228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- `pickle.dump(...)`: Serializes (saves) Python objects to a file in a binary format.\n- `'wb'`: Means write binary.\n- `pickle.load(...)`: Deserializes (loads) the data back from the pickle files into memory.\n- `'rb'`: Means read binary.\n\nResult:\n\n- **(704, 224, 224, 3)**: There are 704 images, each 224×224 pixels in size with 3 color channels (RGB).\n- **(704,)**: The corresponding labels array for these images has 704 labels.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation\n\nWe can evaluate the trained model on the full test dataset (`X_test`, `y_test`) and print its performance metrics. Specifically, we `model.evaluate(...)` to obtain the final **loss** and **accuracy** on unseen data, giving us a measure of how well our model generalizes beyond the training set.","metadata":{}},{"cell_type":"markdown","source":"### Evaluation the model","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint(f'Test loss: {score[0]} / Test accuracy: {score[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:13:39.292922Z","iopub.execute_input":"2025-07-02T14:13:39.293294Z","iopub.status.idle":"2025-07-02T14:13:48.254631Z","shell.execute_reply.started":"2025-07-02T14:13:39.293265Z","shell.execute_reply":"2025-07-02T14:13:48.253675Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above function runs the model on the test data (`X_test` and corresponding labels `y_test`) and returns a list (or tuple) of metrics. In this case, because our model was compiled with `loss='binary_crossentropy'` and `metrics=['accuracy']`, `score[0]` will be the test loss and `score[1]` will be the test accuracy. `verbose=0` means it will not print any progress bar or additional information during evaluation.\n\nTest loss is about **0.48672**, which is a moderate number (lower is typically better). Test accuracy is about **0.97727**, suggesting our model correctly classifies around **97%** of the test samples. This indicates a solid performance on this binary classification task.","metadata":{}},{"cell_type":"markdown","source":"### Evaluating the saved model","metadata":{}},{"cell_type":"code","source":"score = restored_model.evaluate(X_test, y_test, verbose=0)\nprint(f'Test loss: {score[0]} / Test accuracy: {score[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:13:53.317938Z","iopub.execute_input":"2025-07-02T14:13:53.318343Z","iopub.status.idle":"2025-07-02T14:14:04.294647Z","shell.execute_reply.started":"2025-07-02T14:13:53.318311Z","shell.execute_reply":"2025-07-02T14:14:04.293603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evidently, the saved model would yeild the same results.","metadata":{}},{"cell_type":"markdown","source":"## Prediction\n\nWe can use our trained model to generate predicted probabilities for each image in the test set. Since this is a binary classification problem, each number represents the model's estimated likelihood that the image belongs to class **1** (the **Malignant** class).","metadata":{}},{"cell_type":"markdown","source":"### Prediction with the model","metadata":{}},{"cell_type":"code","source":"y_pred_prob = model.predict(X_test)\nprint(y_pred_prob)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:14:10.025011Z","iopub.execute_input":"2025-07-02T14:14:10.025422Z","iopub.status.idle":"2025-07-02T14:14:18.066290Z","shell.execute_reply.started":"2025-07-02T14:14:10.025389Z","shell.execute_reply":"2025-07-02T14:14:18.065054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediction with the saved model","metadata":{}},{"cell_type":"code","source":"y_pred_prob = restored_model.predict(X_test)\nprint(y_pred_prob)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:14:32.597748Z","iopub.execute_input":"2025-07-02T14:14:32.598081Z","iopub.status.idle":"2025-07-02T14:14:48.353964Z","shell.execute_reply.started":"2025-07-02T14:14:32.598055Z","shell.execute_reply":"2025-07-02T14:14:48.350069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's plot two sets of points for each test sample (indexed on the x-axis):\n\n- `y_pred_prob` (red dots): These are the predicted probabilities for each sample. A value near 1 indicates the model believes the image is likely in class **1** (**Malignant**), and a value near 0 indicates the model thinks it's class **0** (**Bengin**).\n- `y_test` (blue dots): These are the actual labels (ground truth), which are 0 or 1 in a binary classification. Here, each blue dot at the top represents an actual label of \"1\" while each blue dot along the bottom represents \"0\"","metadata":{}},{"cell_type":"code","source":"plt.plot(y_pred_prob,'.', color='red', label='Predicted Probabilty')\nplt.plot(y_test, '.', color='navy', label='Actual Labels')\nplt.xlabel('Instance Number')\nplt.ylabel('Probability')\nplt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:14:48.358771Z","iopub.execute_input":"2025-07-02T14:14:48.359141Z","iopub.status.idle":"2025-07-02T14:14:49.758003Z","shell.execute_reply.started":"2025-07-02T14:14:48.359109Z","shell.execute_reply":"2025-07-02T14:14:49.756510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Thresholding\n\nInspecting the Ground Truth Labels:","metadata":{}},{"cell_type":"code","source":"y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:14:49.760390Z","iopub.execute_input":"2025-07-02T14:14:49.760800Z","iopub.status.idle":"2025-07-02T14:14:49.774013Z","shell.execute_reply.started":"2025-07-02T14:14:49.760768Z","shell.execute_reply":"2025-07-02T14:14:49.772706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's apply a threshold of 0.5 on the model's predicted probabilities to get final class predictions `y_pred`.","metadata":{}},{"cell_type":"code","source":"threshold = 0.5\n\ny_pred = np.where(y_pred_prob > threshold, 1 ,0)\ny_pred.squeeze\n\ny_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:14:49.775417Z","iopub.execute_input":"2025-07-02T14:14:49.775953Z","iopub.status.idle":"2025-07-02T14:14:49.804981Z","shell.execute_reply.started":"2025-07-02T14:14:49.775920Z","shell.execute_reply":"2025-07-02T14:14:49.803279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- `threshold = 0.5`: We choose 0.5 as the cutoff. If the predicted probability `y_pred_prob` is above 0.5, we predict class 1, otherwise class 0. This is the standard threshold for binary classification when using a sigmoid output layer.\n- `np.where(y_pred_prob > threshold, 1, 0)`: Converts the continuous probabilities into discrete class predictions (0 or 1).\n- `y_pred.squeeze()`: Removes extra dimensions if present (e.g., if `y_pred` is of shape (N, 1) instead of (N,)).\n- `y_pred`: Now contains integer class predictions for each sample—either 0 or 1.","metadata":{}},{"cell_type":"markdown","source":"## Analysis\n\nWe can visualize the Confusion Matrix as a Heatmap to help us quickly assess the performance of your model, whether it's correctly predicting malignant and benign cases, and how often it makes misclassifications in each direction.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(7.7,6.27)})\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap=plt.cm.Blues, annot=True, annot_kws={\"size\": 32}, fmt='g')\n\nplt.xticks([0.50,1.50], ['Malignant','Benign'], fontsize=20)\nplt.yticks([0.50,1.50],['Malignant','Benign'], fontsize=20)\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title('Confusion Matrix for Breast Cancer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:15:06.226783Z","iopub.execute_input":"2025-07-02T14:15:06.227301Z","iopub.status.idle":"2025-07-02T14:15:06.528982Z","shell.execute_reply.started":"2025-07-02T14:15:06.227259Z","shell.execute_reply":"2025-07-02T14:15:06.527802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# confusion matrix in percentage \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Normalize the confusion matrix to get percentages\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot the heatmap with normalized percentages\nsns.set(rc={'figure.figsize':(7.7,6.27)})\nsns.heatmap(cm_normalized, cmap=plt.cm.Blues, annot=True, fmt='.2%', annot_kws={\"size\": 32})\n\n# Set tick labels with font size\nplt.xticks([0.5, 1.5], ['Malignant', 'Benign'], fontsize=20)\nplt.yticks([0.5, 1.5], ['Malignant', 'Benign'], fontsize=20)\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title('Confusion Matrix for Breast Cancer in Percentage')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:15:11.432923Z","iopub.execute_input":"2025-07-02T14:15:11.433304Z","iopub.status.idle":"2025-07-02T14:15:11.695103Z","shell.execute_reply.started":"2025-07-02T14:15:11.433276Z","shell.execute_reply":"2025-07-02T14:15:11.694156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport pandas as pd\n\n# Generate the classification report as a dictionary\nreport_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Malignant'], output_dict=True)\n\n# Convert the report into a pandas DataFrame\nreport_df = pd.DataFrame(report_dict).transpose()\n\n# Select metrics for classes\nmetrics = report_df.loc[['Benign', 'Malignant'], ['precision', 'recall', 'f1-score', 'support']]\n\n# Extract macro and weighted averages\nmacro_avg = report_df.loc['macro avg', ['precision', 'recall', 'f1-score', 'support']]\nweighted_avg = report_df.loc['weighted avg', ['precision', 'recall', 'f1-score', 'support']]\n\n# Append macro and weighted averages to the metrics DataFrame\nmetrics.loc['Macro Avg'] = macro_avg\nmetrics.loc['Weighted Avg'] = weighted_avg\n\n# Print the table\nprint(metrics)\n\n\n\nprint(\"retry for 100 epochs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:15:15.207142Z","iopub.execute_input":"2025-07-02T14:15:15.207523Z","iopub.status.idle":"2025-07-02T14:15:15.245058Z","shell.execute_reply.started":"2025-07-02T14:15:15.207495Z","shell.execute_reply":"2025-07-02T14:15:15.243972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the above, we see that out of **237 malignant** samples (top row), the model correctly identifies **229** as malignant but misclassifies **8** as benign. For the **467** benign samples (bottom row), **459** are correctly predicted as benign, with **8** mistakenly labeled as malignant. Overall, the model does a good job classifying benign cases (low false positives) but still misses some malignant cases (8 false negatives). In a medical context, those false negatives are critical because they represent malignant tumors misdiagnosed as benign. Nonetheless, the overall accuracy is high, indicating strong performance on the dataset.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred, target_names = ['Benign (Class 0)','Malignant (Class 1)']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:15:20.481707Z","iopub.execute_input":"2025-07-02T14:15:20.482079Z","iopub.status.idle":"2025-07-02T14:15:20.496855Z","shell.execute_reply.started":"2025-07-02T14:15:20.482048Z","shell.execute_reply":"2025-07-02T14:15:20.495743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Precision**: Measures how many of the samples predicted as a certain class (e.g., **Malignant**) actually belong to that class.\n- **Recall**: (Also known as sensitivity) measures how many of the samples belonging to a certain class (e.g., Malignant) are correctly identified.\n- **F1 score**: The harmonic mean of precision and recall. It balances both metrics into one number.\n- **Support**: The number of samples in the dataset belonging to each class.\n- **Accuracy**: The proportion of all samples (both positive and negative) that were correctly classified.\n- **Macro avg**: The metric (e.g., precision, recall, F1) independently for each class, and then takes the average. This treats all classes equally.\n- **Weighted avg**: The metric (e.g., precision, recall, F1) for each class and weights them by the number of samples from that class.​\n\nThe model does particularly well identifying malignant tumors (high recall of 0.98). This is typically desirable in a medical setting, because missing malignant cases (false negatives) can be more critical. However, the recall for benign cases is lower (0.97), meaning some benign samples are misclassified as malignant. Overall, the model is still quite strong in distinguishing the two classes, as shown by an F1-score above 0.98 for malignant and 0.97 for benign.\n\nIf we generate an ROC curve by comparing the model's predicted probabilities `y_pred_prob` against the true labels `y_test` at various decision thresholds we can calculate the false positive rate (FPR) and true positive rate (TPR) for each threshold and then plot them.","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\narea_under_curve = auc(fpr, tpr)\n\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(fpr, tpr, label='AUC = {:.3f}'.format(area_under_curve))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\n\n\n\n\nplt.savefig(\"ROC_Curve.png\")\n\n\nplt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:15:25.119103Z","iopub.execute_input":"2025-07-02T14:15:25.119541Z","iopub.status.idle":"2025-07-02T14:15:25.562580Z","shell.execute_reply.started":"2025-07-02T14:15:25.119507Z","shell.execute_reply":"2025-07-02T14:15:25.561485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Predict probabilities on train and validation sets\ny_train_prob = model.predict(train, verbose=1)\ny_val_prob = model.predict(test, verbose=1)\n\n# Get true binary labels\ny_train_true = train.classes\ny_val_true = test.classes\n\n# Compute ROC curve and AUC for training\nfpr_train, tpr_train, _ = roc_curve(y_train_true, y_train_prob)\nauc_train = auc(fpr_train, tpr_train)\n\n# Compute ROC curve and AUC for validation\nfpr_val, tpr_val, _ = roc_curve(y_val_true, y_val_prob)\nauc_val = auc(fpr_val, tpr_val)\n\n# Plot\nplt.figure(figsize=(10, 7))\nplt.plot(fpr_train, tpr_train, label=f\"Train AUC = {auc_train:.4f}\", color='blue')\nplt.plot(fpr_val, tpr_val, label=f\"Validation AUC = {auc_val:.4f}\", color='green')\n\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\", fontsize=14)\nplt.ylabel(\"True Positive Rate\", fontsize=14)\nplt.title(\"ROC Curve for Train vs Validation\", fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:18:39.904495Z","iopub.execute_input":"2025-07-02T14:18:39.904934Z","iopub.status.idle":"2025-07-02T14:20:05.236498Z","shell.execute_reply.started":"2025-07-02T14:18:39.904897Z","shell.execute_reply":"2025-07-02T14:20:05.235293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The AUC (Area Under the Curve) of 0.947 shows that our model has strong discriminatory power—it can distinguish malignant from benign cases accurately across a broad range of thresholds. The closer the AUC is to 1.0, the better the model's overall performance.\n\nNow, let's sample 25 random test images from the dataset `X_test`. For each image we'll retrieve:\n\n1. The predicted class (either 0 or 1) and mapped it to \"Benign\" or \"Malignant\".\n2. The model's probability for that predicted class.","metadata":{}},{"cell_type":"code","source":"# mapping numeric predictions (0 or 1) to the corresponding string labels.\ncancer_labels = ['Benign', 'Malignant']\n\n# Randomly picking 25 indices from the test set, then retrieving those images.\nimg_indices = np.random.randint(0, len(X_test), size=25)\nsample_test_images = X_test[img_indices]\n\n# Creatinng a list of predicted labels (e.g., \"Benign\" or \"Malignant\") corresponding to each of the chosen images.\nactual_test_labels = [cancer_labels[int(label)] for label in y_test[img_indices]]\n\nmax_prediction = np.max(y_pred_prob, axis=1)\nprediction_probs = np.max(y_pred_prob, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T16:00:05.784138Z","iopub.execute_input":"2025-07-02T16:00:05.787738Z","iopub.status.idle":"2025-07-02T16:00:05.900584Z","shell.execute_reply.started":"2025-07-02T16:00:05.787519Z","shell.execute_reply":"2025-07-02T16:00:05.899472Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- `max_prediction`: For each test sample, takes the index of the highest probability. (In a binary classification with one output neuron, this step usually isn't necessary—if you had two output neurons, it picks whichever neuron has the higher probability.)\n- `prediction_probs`: Grabs the actual probability value of that highest-probability class.\n\nWe then display each image in a subplot, labeling it with the predicted class and its predicted probability on the x-axis, and (in this snippet) also showing the predicted label again on the y-axis.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\nfor i, (img, pred_idx, prob, true_label) in enumerate(\n    zip(sample_test_images, max_prediction[img_indices], \n        prediction_probs[img_indices], actual_test_labels)\n):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n\n    plt.imshow(img)\n    plt.xlabel(f\"{cancer_labels[int(round(pred_idx))]} ({prob:.3f})\")\n    plt.ylabel(true_label)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:15:41.970980Z","iopub.execute_input":"2025-07-02T14:15:41.971411Z","iopub.status.idle":"2025-07-02T14:15:45.073063Z","shell.execute_reply.started":"2025-07-02T14:15:41.971380Z","shell.execute_reply":"2025-07-02T14:15:45.071761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By visually inspecting these images, you can see how confident the model is for each prediction. If \"Benign\" images mostly have predicted probabilities close to 1.0 for \"Benign\", and \"Malignant\" images similarly have high probabilities for \"Malignant\" the model is performing well. In the provided screenshot, most if not all predicted labels and confidence levels appear correct or strongly confident. However, any discrepancies (e.g., a visually \"Malignant\" image labeled as \"Benign\" with a high probability) would signal a misclassification worth investigating.","metadata":{}},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}