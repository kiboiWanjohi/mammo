{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ugD6gz--CROj"
      },
      "source": [
        "# Breast Cancer Detection With Mammograms\n",
        "\n",
        "This is a demonstration on how to build a model that gives prediction on breast cancer given a set of mammograms.\n",
        "\n",
        "The dataset used is the [Augmented INBreast Dataset](https://www.kaggle.com/datasets/eoussama/breast-cancer-mammograms).\n",
        "\n",
        "A mirror repository for this notebook can be found on [github](https://github.com/gomu-gomu/ma-dl-projet-1).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ueRFne-WCWvh",
        "outputId": "080ffa63-266f-44da-ee68-3f98a0605dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "dmGC6XeBCXEI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/Stat_Docs/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ilNly2grCXGo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "NAU0d3nPCXJX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download eoussama/breast-cancer-mammograms"
      ],
      "metadata": {
        "id": "_l5U_oF3CXLw",
        "outputId": "26da3bd0-5b0d-4803-b6e5-2551e667b717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/eoussama/breast-cancer-mammograms\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading breast-cancer-mammograms.zip to /content\n",
            "  0% 0.00/73.0M [00:00<?, ?B/s]\n",
            "100% 73.0M/73.0M [00:00<00:00, 1.14GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/breast-cancer-mammograms.zip -d /content/ > /dev/null"
      ],
      "metadata": {
        "id": "u2VMQejXCXSR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! df -h"
      ],
      "metadata": {
        "id": "zJAjDizWCXUx",
        "outputId": "2c7df433-b480-47bd-bbe3-783f98ebb395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   38G   70G  36% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  775M  61% /usr/sbin/docker-init\n",
            "/dev/sda1        73G   39G   34G  54% /kaggle/input\n",
            "tmpfs           6.4G  524K  6.4G   1% /var/colab\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "drive            15G  7.1G  8.0G  48% /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv6w3Pv8CROk"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:25:31.752835Z",
          "iopub.status.busy": "2025-07-02T10:25:31.752493Z",
          "iopub.status.idle": "2025-07-02T10:26:56.770448Z",
          "shell.execute_reply": "2025-07-02T10:26:56.769324Z",
          "shell.execute_reply.started": "2025-07-02T10:25:31.752802Z"
        },
        "id": "sOEodHkcCROk",
        "outputId": "b340472b-3e68-4195-8797-03486960642b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.73.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:26:56.771705Z",
          "iopub.status.busy": "2025-07-02T10:26:56.771403Z",
          "iopub.status.idle": "2025-07-02T10:27:04.132895Z",
          "shell.execute_reply": "2025-07-02T10:27:04.132083Z",
          "shell.execute_reply.started": "2025-07-02T10:26:56.771678Z"
        },
        "id": "2NZHPra1CROl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_3uSmawCROl"
      },
      "source": [
        "## Dataset INBreast Dataset\n",
        "We have to prepare our dataset and split it in a convenient way. First we start by setting up separate folders fore each sub-category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:27:04.134617Z",
          "iopub.status.busy": "2025-07-02T10:27:04.133842Z",
          "iopub.status.idle": "2025-07-02T10:27:40.608328Z",
          "shell.execute_reply": "2025-07-02T10:27:40.607404Z",
          "shell.execute_reply.started": "2025-07-02T10:27:04.134578Z"
        },
        "id": "xtwnCwf8CROl"
      },
      "outputs": [],
      "source": [
        "output_path = \"/colab/working\"\n",
        "input_path = \"/colab/input/breast-cancer-mammograms/\"\n",
        "data_path = f\"{output_path}/split_data\"\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for cls in ['benign', 'malignant']:\n",
        "        os.makedirs(os.path.join(data_path, split, cls), exist_ok=True)\n",
        "\n",
        "def get_files(cls):\n",
        "    return [f for f in os.listdir(os.path.join(input_path, cls))\n",
        "           if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "benign_files = get_files('benign')\n",
        "malignant_files = get_files('malignant')\n",
        "\n",
        "def split_data(files):\n",
        "    train, temp = train_test_split(files, test_size=0.2, random_state=42)\n",
        "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "    return train, val, test\n",
        "\n",
        "benign_train, benign_val, benign_test = split_data(benign_files)\n",
        "malignant_train, malignant_val, malignant_test = split_data(malignant_files)\n",
        "\n",
        "def copy_files(files, cls, split):\n",
        "    for f in files:\n",
        "        src = os.path.join(input_path, cls, f)\n",
        "        dst = os.path.join(data_path, split, cls, f)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "copy_files(benign_train, 'benign', 'train')\n",
        "copy_files(benign_val, 'benign', 'val')\n",
        "copy_files(benign_test, 'benign', 'test')\n",
        "\n",
        "copy_files(malignant_train, 'malignant', 'train')\n",
        "copy_files(malignant_val, 'malignant', 'val')\n",
        "copy_files(malignant_test, 'malignant', 'test')\n",
        "\n",
        "print(\"Dataset split completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk_FZPqDCROm"
      },
      "source": [
        "### Loading the data\n",
        "Then we create an image data generator that will scale each pixel's value by 1/255, essentially normalizing them from the standard 0–255 range to a 0–1 range. Then we load the three data collections separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:27:40.610478Z",
          "iopub.status.busy": "2025-07-02T10:27:40.610165Z",
          "iopub.status.idle": "2025-07-02T10:27:40.752834Z",
          "shell.execute_reply": "2025-07-02T10:27:40.751832Z",
          "shell.execute_reply.started": "2025-07-02T10:27:40.610453Z"
        },
        "id": "jPY1FIeUCROm"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train = datagen.flow_from_directory(\n",
        "    f'{data_path}/train',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='binary',\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "val = datagen.flow_from_directory(\n",
        "    f'{data_path}/val',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='binary',\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "test = datagen.flow_from_directory(\n",
        "    f'{data_path}/test',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='binary',\n",
        "    batch_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ_TXyytCROm"
      },
      "source": [
        "### Data inspection\n",
        "Now we fetch the first batch of images and labels out of `train`, and confirm their shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:27:40.754570Z",
          "iopub.status.busy": "2025-07-02T10:27:40.754199Z",
          "iopub.status.idle": "2025-07-02T10:27:40.903242Z",
          "shell.execute_reply": "2025-07-02T10:27:40.902292Z",
          "shell.execute_reply.started": "2025-07-02T10:27:40.754535Z"
        },
        "id": "2SA3XWrZCROm"
      },
      "outputs": [],
      "source": [
        "imgs, labels = next(train)\n",
        "imgs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHp4X2ZfCROn"
      },
      "source": [
        "* **64**: the batch size (number of images).\n",
        "* **224 × 224**: the height and width of each image, as defined in `target_size=(224, 224)`.\n",
        "* **3**: the number of color channels (RGB).\n",
        "\n",
        "We confirm that we only have 2 classes. **Benign** and **Malgnant**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:27:40.904495Z",
          "iopub.status.busy": "2025-07-02T10:27:40.904219Z",
          "iopub.status.idle": "2025-07-02T10:27:40.910109Z",
          "shell.execute_reply": "2025-07-02T10:27:40.909103Z",
          "shell.execute_reply.started": "2025-07-02T10:27:40.904472Z"
        },
        "id": "XkjHqabTCROn"
      },
      "outputs": [],
      "source": [
        "train.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuvSLZonCROn"
      },
      "source": [
        "Displaying the first image in the training dataset should show us a grayscale scan of a breast, accompanied with the correct class decided by its diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:27:40.911910Z",
          "iopub.status.busy": "2025-07-02T10:27:40.911493Z",
          "iopub.status.idle": "2025-07-02T10:27:41.209398Z",
          "shell.execute_reply": "2025-07-02T10:27:41.208436Z",
          "shell.execute_reply.started": "2025-07-02T10:27:40.911873Z"
        },
        "id": "tpgQDhMXCROn"
      },
      "outputs": [],
      "source": [
        "plt.imshow(imgs[0])\n",
        "print(f'Class: {labels[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7t_M8LUCROn"
      },
      "source": [
        "The class is `1.0`, which matches **Malignant** on the class indices of the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGA482PCROn"
      },
      "source": [
        "## Model\n",
        "\n",
        "We're gonna build a simple Convolutional Neural Network (CNN) for binary classification. First, we add three convolution-and-pooling blocks to extract spatial features from the images. Then, we flatten the feature maps and pass them through a couple of dense (fully connected) layers, including a dropout for regularization. Finally, we use a single output neuron with a sigmoid activation for predicting whether an image is benign or malignant.\n",
        "\n",
        "* Step 1:\n",
        "\n",
        "    - We start by initializing a new `Sequential` model, which is a linear stack of layers in Keras.\n",
        "* Step 2:\n",
        "\n",
        "    - `Conv2D(32, (3, 3))`: A convolutional layer with 32 filters/kernels, each 3×3 in size.\n",
        "    - `input_shape=(224,224,3)`: This is the expected shape of the input images. 224×224 resolution, 3 color channels (RGB).\n",
        "    - `activation='relu'`: Uses the ReLU activation function.\n",
        "    - `MaxPooling2D(pool_size=(2, 2))`: Reduces the spatial dimensions by taking the maximum value in each 2×2 pool region, effectively halving the height and width.\n",
        "* Step 3:\n",
        "\n",
        "    - Similar to the first block, but it doesn't need an input_shape since Keras automatically infers the shape from the previous layer's output.\n",
        "    - Another 32 filters, each 3×3, followed by a 2×2 max pool to further downsample.\n",
        "* Step 4:\n",
        "\n",
        "    - Now we increase the number of filters to 64. A larger number of filters can learn more complex features.\n",
        "    - Followed by another 2×2 max pool for further spatial reduction.\n",
        "* Step 5:\n",
        "\n",
        "    - `Flatten()`: Converts the 3D feature maps (height × width × channels) into a 1D vector, so it can be fed into a dense (fully connected) layer.\n",
        "    - `Dense(64, activation='relu')`: A fully connected layer with 64 neurons, learning high-level combinations of features.\n",
        "    - `Dropout(0.5)`: Randomly sets 50% of the neurons to zero during training to reduce overfitting.\n",
        "    - `Dense(1, activation='sigmoid')`: The final output layer. Since this is a binary classification (benign vs. malignant), we use a single neuron with a sigmoid activation, which outputs a probability between 0 and 1.\n",
        "* Step 6:\n",
        "\n",
        "    - `loss='binary_crossentropy'`: The appropriate loss function for binary classification.\n",
        "    - `optimizer='rmsprop'`: A gradient-based optimization algorithm (alternative to Adam, SGD, etc.).\n",
        "    - `metrics=['accuracy']`: We track accuracy during training/evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:31:22.029222Z",
          "iopub.status.busy": "2025-07-02T10:31:22.027918Z",
          "iopub.status.idle": "2025-07-02T10:31:22.208213Z",
          "shell.execute_reply": "2025-07-02T10:31:22.207236Z",
          "shell.execute_reply.started": "2025-07-02T10:31:22.029155Z"
        },
        "id": "Z5uK3sISCROn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "\n",
        "\n",
        "# Step 1 - Initializing model\n",
        "model = Sequential()\n",
        "\n",
        "# Step 2 - First Convolution + MaxPooling\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(224,224,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Step 3 - Second Convolution + MaxPooling\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Step 4 - Third Convolution + MaxPooling\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Step 5 - Flatten + Dense + Dropout + Output\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Step 6 - Compile\n",
        "# Step 6 - Compile\n",
        "\n",
        "\n",
        "# additional metrics\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy', AUC(name='auc'), Precision(name='precision'), Recall(name='recall')])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAInJbo0CROo"
      },
      "source": [
        "- **Output Shape**: The size of the activation maps after each layer.\n",
        "\n",
        "- **Param #**: The number of trainable parameters (weights and biases).\n",
        "\n",
        "> Notice that most of the parameters (over 2.7 million) are in the fully connected layer (dense_6), which converts the flattened feature maps to 64 neurons.\n",
        "\n",
        "- **Total params**: The sum of all parameters in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGboJMj9CROo"
      },
      "source": [
        "## Training\n",
        "\n",
        "In this step we will train the CNN for 100 epochs while logging metrics to a CSV file (training.log) for analysis, and then saves the trained model for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T10:31:40.667786Z",
          "iopub.status.busy": "2025-07-02T10:31:40.667408Z",
          "iopub.status.idle": "2025-07-02T14:07:02.256095Z",
          "shell.execute_reply": "2025-07-02T14:07:02.252882Z",
          "shell.execute_reply.started": "2025-07-02T10:31:40.667758Z"
        },
        "id": "Y6evHM8jCROo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Calculating how many batches (steps) makeup one full pass (epoch) through our training and validation datasets.\n",
        "STEP_SIZE_TRAIN = train.n // train.batch_size\n",
        "STEP_SIZE_VAL = val.n // val.batch_size\n",
        "\n",
        "os.makedirs(os.path.join(output_path, 'logs'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_path, 'out'), exist_ok=True)\n",
        "\n",
        "# This callback records loss, accuracy, and other metrics for each epoch into a CSV file named training.log.\n",
        "# It's helpful for tracking and comparing training progress over time.\n",
        "csv_logger = CSVLogger(f'{output_path}/logs/training.log', separator=',', append=False)\n",
        "\n",
        "history = model.fit_generator(generator = train,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=test,\n",
        "                    validation_steps=STEP_SIZE_VAL,\n",
        "                    epochs = 50,\n",
        "                    # epochs=100,\n",
        "                    callbacks=[csv_logger])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.save(f'{output_path}/out/model.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQHeS_9qCROo"
      },
      "source": [
        "To determine the step sizes, take note of the following:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- `train.n` and `val.n`: The total number of samples in each dataset.\n",
        "- `train.batch_size` and `val.batch_size`: The batch size.\n",
        "- Using integer division `//` gives you the number of batches needed.\n",
        "\n",
        "As for fitting the model:\n",
        "- `model.fit_generator(...)`: Trains the model using the data generated from train (the training image generator) in batches.\n",
        "- `steps_per_epoch=STEP_SIZE_TRAIN`: How many steps (batches) to run per epoch during training.\n",
        "- `validation_data=test` and `validation_steps=STEP_SIZE_VAL`: Here, you're using the test generator for validation, with STEP_SIZE_VAL batches per epoch.\n",
        "- `epochs=100`: The number of times the model will see the entire dataset.\n",
        "- `callbacks=[csv_logger]`: Logs each epoch's metrics to the CSV file.\n",
        "\n",
        "We can visualize how our model's accuracy changes over the course of training. By plotting both the training and validation accuracy, we'll get a clear picture of how well the model is fitting to the training data and how effectively it generalizes to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:07:09.969810Z",
          "iopub.status.busy": "2025-07-02T14:07:09.969317Z",
          "iopub.status.idle": "2025-07-02T14:07:10.432359Z",
          "shell.execute_reply": "2025-07-02T14:07:10.431089Z",
          "shell.execute_reply.started": "2025-07-02T14:07:09.969750Z"
        },
        "id": "p829gqcbCROo"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.plot(np.arange(1, len(history.history['accuracy'])+1,1), history.history['accuracy'], color='navy', label = 'Accuracy')\n",
        "plt.plot(np.arange(1, len(history.history['accuracy'])+1,1), history.history['val_accuracy'], color='red', label='Validation Accuracy')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKXrSKPxCROo"
      },
      "source": [
        "The plot shows two lines: one for the training **accuracy** (in navy) and another for the validation accuracy (in red). As the number of epochs increases, we can observe whether the model converges, if it overfits (training accuracy outpacing validation accuracy), or if both accuracies improve steadily over time. This helps us assess the model's performance and decide on further tuning.\n",
        "\n",
        "From the plot, we can see that the training accuracy (blue line) reaches near-perfect levels, while the validation accuracy (red line) plateaus around the high 80s to low 90s. This suggests the model is learning effectively but also overfitting somewhat—its performance on unseen data (validation accuracy) isn't as high as on the training set. Even so, the validation accuracy still remains robust, indicating that the model generally performs well at classifying new examples despite not matching the near-perfect training performance.\n",
        "\n",
        "We can do the same with the saved model. We can read back the CSV file (training.log) that was generated by the `CSVLogger` during training. Each row in this file corresponds to an epoch, and the columns contain metrics like loss, accuracy, validation loss, and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:07:17.816145Z",
          "iopub.status.busy": "2025-07-02T14:07:17.815801Z",
          "iopub.status.idle": "2025-07-02T14:07:17.908805Z",
          "shell.execute_reply": "2025-07-02T14:07:17.907637Z",
          "shell.execute_reply.started": "2025-07-02T14:07:17.816118Z"
        },
        "id": "04Hmq0hOCROp"
      },
      "outputs": [],
      "source": [
        "log_data = pd.read_csv(f'{output_path}/logs/training.log', sep=',', engine='python')\n",
        "log_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:07:24.883713Z",
          "iopub.status.busy": "2025-07-02T14:07:24.883282Z",
          "iopub.status.idle": "2025-07-02T14:07:25.243999Z",
          "shell.execute_reply": "2025-07-02T14:07:25.242848Z",
          "shell.execute_reply.started": "2025-07-02T14:07:24.883684Z"
        },
        "id": "Ss2IeJ8TCROp"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.plot(np.arange(1, len(log_data['accuracy'])+1,1), log_data['accuracy'], color='navy', label = 'Accuracy')\n",
        "plt.plot(np.arange(1, len(log_data['accuracy'])+1,1), log_data['val_accuracy'], color='red', label='Validation Accuracy')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pogVL9lDCROp"
      },
      "source": [
        "Again, visualizing how our model's **training loss** (in navy) compares to the **validation loss** (in red) over each epoch of training. By charting both curves, we can spot whether the model is successfully generalizing (both losses decreasing together) or if it begins to overfit (training loss keeps going down while validation loss rises). This step is essential to ensure our model is learning appropriately and to guide further tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:07:30.089019Z",
          "iopub.status.busy": "2025-07-02T14:07:30.088614Z",
          "iopub.status.idle": "2025-07-02T14:07:30.430233Z",
          "shell.execute_reply": "2025-07-02T14:07:30.429230Z",
          "shell.execute_reply.started": "2025-07-02T14:07:30.088970Z"
        },
        "id": "GBCveR_nCROp"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.plot(np.arange(1, len(history.history['loss'])+1,1), history.history['loss'], color='navy', label = 'Loss')\n",
        "plt.plot(np.arange(1, len(history.history['loss'])+1,1), history.history['val_loss'], color='red', label='Validation Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5T8pCnCROp"
      },
      "source": [
        "From this plot, it's clear that the **training loss** (blue) steadily goes down to very low values, indicating the model is fitting the training data extremely well. However, the **validation loss** (red) varies up and down rather than following the same steady downward trend, suggesting the model is **overfitting** and not generalizing as consistently to unseen data. The spikes in the validation loss can occur for various reasons (e.g., data variability, small validation sets, or model instability), but the key takeaway is that while the model memorizes the training set effectively, its performance on new data is less stable.\n",
        "\n",
        "Doing the same again with the logged data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:07:34.797999Z",
          "iopub.status.busy": "2025-07-02T14:07:34.797626Z",
          "iopub.status.idle": "2025-07-02T14:07:35.145115Z",
          "shell.execute_reply": "2025-07-02T14:07:35.143984Z",
          "shell.execute_reply.started": "2025-07-02T14:07:34.797969Z"
        },
        "id": "3L5VAODzCROp"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.plot(np.arange(1, len(log_data['loss'])+1,1), log_data['loss'], color='navy', label = 'Loss')\n",
        "plt.plot(np.arange(1, len(log_data['loss'])+1,1), log_data['val_loss'], color='red', label='Validation Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-daaafLCROp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:07:39.607407Z",
          "iopub.status.busy": "2025-07-02T14:07:39.607024Z",
          "iopub.status.idle": "2025-07-02T14:07:49.773215Z",
          "shell.execute_reply": "2025-07-02T14:07:49.771942Z",
          "shell.execute_reply.started": "2025-07-02T14:07:39.607377Z"
        },
        "id": "fvtMuxoNCROp"
      },
      "outputs": [],
      "source": [
        "# print classification report before hibernation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the trained model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(f'{output_path}/out/model.h5')\n",
        "\n",
        "# Step 2: Prepare the test data generator\n",
        "test_generator = test  # Your existing test data generator\n",
        "\n",
        "# Step 3: Predict the probabilities on the test set\n",
        "# Note: Use predict() with batch size as needed\n",
        "pred_probs = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
        "# For binary classification, convert probabilities to class labels\n",
        "pred_labels = (pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "# Step 4: Get true labels\n",
        "true_labels = test_generator.classes\n",
        "\n",
        "# Step 5: Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, target_names=['benign', 'malignant'])\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Optional: Confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:08:09.760189Z",
          "iopub.status.busy": "2025-07-02T14:08:09.759807Z",
          "iopub.status.idle": "2025-07-02T14:08:10.032752Z",
          "shell.execute_reply": "2025-07-02T14:08:10.031689Z",
          "shell.execute_reply.started": "2025-07-02T14:08:09.760138Z"
        },
        "id": "oLtqZZf2CROq"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:08:15.304700Z",
          "iopub.status.busy": "2025-07-02T14:08:15.304303Z",
          "iopub.status.idle": "2025-07-02T14:08:15.567045Z",
          "shell.execute_reply": "2025-07-02T14:08:15.566094Z",
          "shell.execute_reply.started": "2025-07-02T14:08:15.304664Z"
        },
        "id": "sR_4URGdCROq"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Normalize the confusion matrix to percentages\n",
        "cm_percentage = (cm.astype('float') / cm.sum()) * 100\n",
        "\n",
        "# Plotting the confusion matrix as percentages\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix in Percentages')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:11:54.892128Z",
          "iopub.status.busy": "2025-07-02T14:11:54.891699Z",
          "iopub.status.idle": "2025-07-02T14:11:54.928064Z",
          "shell.execute_reply": "2025-07-02T14:11:54.926534Z",
          "shell.execute_reply.started": "2025-07-02T14:11:54.892098Z"
        },
        "id": "tlRhja7ECROq"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:09:05.783921Z",
          "iopub.status.busy": "2025-07-02T14:09:05.783520Z",
          "iopub.status.idle": "2025-07-02T14:09:09.022298Z",
          "shell.execute_reply": "2025-07-02T14:09:09.021121Z",
          "shell.execute_reply.started": "2025-07-02T14:09:05.783891Z"
        },
        "id": "FuJqc3NSCROq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot AUC\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['auc'], label='Train AUC')\n",
        "plt.plot(history.history['val_auc'], label='Val AUC')\n",
        "plt.title('AUC over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"AUC_Epochs.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['precision'], label='Train Precision')\n",
        "plt.plot(history.history['val_precision'], label='Val Precision')\n",
        "plt.title('Precision over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"Precision_Epochs.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot Recall\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['recall'], label='Train Recall')\n",
        "plt.plot(history.history['val_recall'], label='Val Recall')\n",
        "plt.title('Recall over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"Recall_Epochs.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"Accuracy_Epochs.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:12:14.400115Z",
          "iopub.status.busy": "2025-07-02T14:12:14.399775Z",
          "iopub.status.idle": "2025-07-02T14:12:24.527314Z",
          "shell.execute_reply": "2025-07-02T14:12:24.526310Z",
          "shell.execute_reply.started": "2025-07-02T14:12:14.400082Z"
        },
        "id": "gXTeFL34CROq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict on all validation data\n",
        "y_pred_prob = model.predict(test, verbose=1)\n",
        "\n",
        "# Class prediction (binary threshold)\n",
        "y_pred_class = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Ground truth labels\n",
        "y_true = test.classes  # length = test.n\n",
        "\n",
        "# Ensure alignment (optional but safe)\n",
        "assert len(y_true) == len(y_pred_class)\n",
        "\n",
        "# Metrics\n",
        "precision = precision_score(y_true, y_pred_class)\n",
        "recall = recall_score(y_true, y_pred_class)\n",
        "f1 = f1_score(y_true, y_pred_class)\n",
        "auc_score = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall:    {recall:.4f}\")\n",
        "print(f\"Test F1 Score:  {f1:.4f}\")\n",
        "print(f\"Test AUC:       {auc_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:12:44.461534Z",
          "iopub.status.busy": "2025-07-02T14:12:44.461114Z",
          "iopub.status.idle": "2025-07-02T14:12:44.772718Z",
          "shell.execute_reply": "2025-07-02T14:12:44.771680Z",
          "shell.execute_reply.started": "2025-07-02T14:12:44.461502Z"
        },
        "id": "9vB-IS9GCROq"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx0pMGizCROq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zSI7VuRCROr"
      },
      "source": [
        "## Hibernation\n",
        "\n",
        "We can load our previously saved model from disk, then extract all the images and labels from our test generator into NumPy arrays. This prepares our entire test set in a convenient format for further evaluation or predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:13:27.794668Z",
          "iopub.status.busy": "2025-07-02T14:13:27.794269Z",
          "iopub.status.idle": "2025-07-02T14:13:29.621760Z",
          "shell.execute_reply": "2025-07-02T14:13:29.620826Z",
          "shell.execute_reply.started": "2025-07-02T14:13:27.794638Z"
        },
        "id": "jB78gDJeCROr"
      },
      "outputs": [],
      "source": [
        "# Loading the previously saved model from the specified path. This includes the architecture, weights, and training configuration (if any).\n",
        "restored_model = load_model(f'{output_path}/out/model.h5')\n",
        "\n",
        "# Calculating how many batches you need to go through in order to cover our entire test set\n",
        "steps = test.n//test.batch_size\n",
        "\n",
        "# Reseting the test generator to start yielding batches from the first image again. This is useful if the generator's internal index was advanced by previous calls (e.g., during validation).\n",
        "test.reset()\n",
        "\n",
        "# Collecting the test data\n",
        "X_test, y_test = [], []\n",
        "for i in range(steps):\n",
        "    a , b = test.next()\n",
        "    X_test.extend(a)\n",
        "    y_test.extend(b)\n",
        "\n",
        "# Converting lists to NumPy arrays\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBEGveeCROr"
      },
      "source": [
        "We can save our test data arrays (X_test and y_test) to disk using `pickle.dump` and then reload them with `pickle.load`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:13:33.292992Z",
          "iopub.status.busy": "2025-07-02T14:13:33.292635Z",
          "iopub.status.idle": "2025-07-02T14:13:34.459154Z",
          "shell.execute_reply": "2025-07-02T14:13:34.458228Z",
          "shell.execute_reply.started": "2025-07-02T14:13:33.292963Z"
        },
        "id": "-EPTu4SaCROr"
      },
      "outputs": [],
      "source": [
        "# Pickling the model\n",
        "pickle.dump(X_test, open(f'{output_path}/out/X_test.pkl', 'wb'))\n",
        "pickle.dump(y_test, open(f'{output_path}/out/y_test.pkl', 'wb'))\n",
        "\n",
        "# Reloading the model to ensure not thing is corrupt\n",
        "X_test = pickle.load(open(f'{output_path}/out/X_test.pkl', 'rb'))\n",
        "y_test = pickle.load(open(f'{output_path}/out/y_test.pkl', 'rb'))\n",
        "\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WtOmz1aCROr"
      },
      "source": [
        "- `pickle.dump(...)`: Serializes (saves) Python objects to a file in a binary format.\n",
        "- `'wb'`: Means write binary.\n",
        "- `pickle.load(...)`: Deserializes (loads) the data back from the pickle files into memory.\n",
        "- `'rb'`: Means read binary.\n",
        "\n",
        "Result:\n",
        "\n",
        "- **(704, 224, 224, 3)**: There are 704 images, each 224×224 pixels in size with 3 color channels (RGB).\n",
        "- **(704,)**: The corresponding labels array for these images has 704 labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beXY_ZGMCROr"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We can evaluate the trained model on the full test dataset (`X_test`, `y_test`) and print its performance metrics. Specifically, we `model.evaluate(...)` to obtain the final **loss** and **accuracy** on unseen data, giving us a measure of how well our model generalizes beyond the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWcqjMM3CROs"
      },
      "source": [
        "### Evaluation the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:13:39.293294Z",
          "iopub.status.busy": "2025-07-02T14:13:39.292922Z",
          "iopub.status.idle": "2025-07-02T14:13:48.254631Z",
          "shell.execute_reply": "2025-07-02T14:13:48.253675Z",
          "shell.execute_reply.started": "2025-07-02T14:13:39.293265Z"
        },
        "id": "vDoOA8aPCROs"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSg3thTUCROs"
      },
      "source": [
        "The above function runs the model on the test data (`X_test` and corresponding labels `y_test`) and returns a list (or tuple) of metrics. In this case, because our model was compiled with `loss='binary_crossentropy'` and `metrics=['accuracy']`, `score[0]` will be the test loss and `score[1]` will be the test accuracy. `verbose=0` means it will not print any progress bar or additional information during evaluation.\n",
        "\n",
        "Test loss is about **0.48672**, which is a moderate number (lower is typically better). Test accuracy is about **0.97727**, suggesting our model correctly classifies around **97%** of the test samples. This indicates a solid performance on this binary classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIobCqlUCROs"
      },
      "source": [
        "### Evaluating the saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:13:53.318343Z",
          "iopub.status.busy": "2025-07-02T14:13:53.317938Z",
          "iopub.status.idle": "2025-07-02T14:14:04.294647Z",
          "shell.execute_reply": "2025-07-02T14:14:04.293603Z",
          "shell.execute_reply.started": "2025-07-02T14:13:53.318311Z"
        },
        "id": "AgqWrGmsCROs"
      },
      "outputs": [],
      "source": [
        "score = restored_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubfv1gh-CROs"
      },
      "source": [
        "Evidently, the saved model would yeild the same results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQRPNUlVCROs"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "We can use our trained model to generate predicted probabilities for each image in the test set. Since this is a binary classification problem, each number represents the model's estimated likelihood that the image belongs to class **1** (the **Malignant** class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELb9Tg0eCROs"
      },
      "source": [
        "### Prediction with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:14:10.025422Z",
          "iopub.status.busy": "2025-07-02T14:14:10.025011Z",
          "iopub.status.idle": "2025-07-02T14:14:18.066290Z",
          "shell.execute_reply": "2025-07-02T14:14:18.065054Z",
          "shell.execute_reply.started": "2025-07-02T14:14:10.025389Z"
        },
        "id": "Xaq6kdrfCROt"
      },
      "outputs": [],
      "source": [
        "y_pred_prob = model.predict(X_test)\n",
        "print(y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp9KwtnKCROt"
      },
      "source": [
        "### Prediction with the saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:14:32.598081Z",
          "iopub.status.busy": "2025-07-02T14:14:32.597748Z",
          "iopub.status.idle": "2025-07-02T14:14:48.353964Z",
          "shell.execute_reply": "2025-07-02T14:14:48.350069Z",
          "shell.execute_reply.started": "2025-07-02T14:14:32.598055Z"
        },
        "id": "zirdigDoCROt"
      },
      "outputs": [],
      "source": [
        "y_pred_prob = restored_model.predict(X_test)\n",
        "print(y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tRpd2fzCROt"
      },
      "source": [
        "Let's plot two sets of points for each test sample (indexed on the x-axis):\n",
        "\n",
        "- `y_pred_prob` (red dots): These are the predicted probabilities for each sample. A value near 1 indicates the model believes the image is likely in class **1** (**Malignant**), and a value near 0 indicates the model thinks it's class **0** (**Bengin**).\n",
        "- `y_test` (blue dots): These are the actual labels (ground truth), which are 0 or 1 in a binary classification. Here, each blue dot at the top represents an actual label of \"1\" while each blue dot along the bottom represents \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:14:48.359141Z",
          "iopub.status.busy": "2025-07-02T14:14:48.358771Z",
          "iopub.status.idle": "2025-07-02T14:14:49.758003Z",
          "shell.execute_reply": "2025-07-02T14:14:49.756510Z",
          "shell.execute_reply.started": "2025-07-02T14:14:48.359109Z"
        },
        "id": "U8J8t-C4CROt"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_prob,'.', color='red', label='Predicted Probabilty')\n",
        "plt.plot(y_test, '.', color='navy', label='Actual Labels')\n",
        "plt.xlabel('Instance Number')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP-ituG4CROt"
      },
      "source": [
        "## Thresholding\n",
        "\n",
        "Inspecting the Ground Truth Labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:14:49.760800Z",
          "iopub.status.busy": "2025-07-02T14:14:49.760390Z",
          "iopub.status.idle": "2025-07-02T14:14:49.774013Z",
          "shell.execute_reply": "2025-07-02T14:14:49.772706Z",
          "shell.execute_reply.started": "2025-07-02T14:14:49.760768Z"
        },
        "id": "IK4XvCrxCROt"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpKgvKwpCROt"
      },
      "source": [
        "Let's apply a threshold of 0.5 on the model's predicted probabilities to get final class predictions `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:14:49.775953Z",
          "iopub.status.busy": "2025-07-02T14:14:49.775417Z",
          "iopub.status.idle": "2025-07-02T14:14:49.804981Z",
          "shell.execute_reply": "2025-07-02T14:14:49.803279Z",
          "shell.execute_reply.started": "2025-07-02T14:14:49.775920Z"
        },
        "id": "lQHvME_bCROu"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "\n",
        "y_pred = np.where(y_pred_prob > threshold, 1 ,0)\n",
        "y_pred.squeeze\n",
        "\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IejHuWzCROu"
      },
      "source": [
        "- `threshold = 0.5`: We choose 0.5 as the cutoff. If the predicted probability `y_pred_prob` is above 0.5, we predict class 1, otherwise class 0. This is the standard threshold for binary classification when using a sigmoid output layer.\n",
        "- `np.where(y_pred_prob > threshold, 1, 0)`: Converts the continuous probabilities into discrete class predictions (0 or 1).\n",
        "- `y_pred.squeeze()`: Removes extra dimensions if present (e.g., if `y_pred` is of shape (N, 1) instead of (N,)).\n",
        "- `y_pred`: Now contains integer class predictions for each sample—either 0 or 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp22IdCTCROu"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "We can visualize the Confusion Matrix as a Heatmap to help us quickly assess the performance of your model, whether it's correctly predicting malignant and benign cases, and how often it makes misclassifications in each direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:15:06.227301Z",
          "iopub.status.busy": "2025-07-02T14:15:06.226783Z",
          "iopub.status.idle": "2025-07-02T14:15:06.528982Z",
          "shell.execute_reply": "2025-07-02T14:15:06.527802Z",
          "shell.execute_reply.started": "2025-07-02T14:15:06.227259Z"
        },
        "id": "GY0-AetHCROu"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(7.7,6.27)})\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), cmap=plt.cm.Blues, annot=True, annot_kws={\"size\": 32}, fmt='g')\n",
        "\n",
        "plt.xticks([0.50,1.50], ['Malignant','Benign'], fontsize=20)\n",
        "plt.yticks([0.50,1.50],['Malignant','Benign'], fontsize=20)\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.title('Confusion Matrix for Breast Cancer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:15:11.433304Z",
          "iopub.status.busy": "2025-07-02T14:15:11.432923Z",
          "iopub.status.idle": "2025-07-02T14:15:11.695103Z",
          "shell.execute_reply": "2025-07-02T14:15:11.694156Z",
          "shell.execute_reply.started": "2025-07-02T14:15:11.433276Z"
        },
        "id": "7De07AyTCROu"
      },
      "outputs": [],
      "source": [
        "# confusion matrix in percentage\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Normalize the confusion matrix to get percentages\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot the heatmap with normalized percentages\n",
        "sns.set(rc={'figure.figsize':(7.7,6.27)})\n",
        "sns.heatmap(cm_normalized, cmap=plt.cm.Blues, annot=True, fmt='.2%', annot_kws={\"size\": 32})\n",
        "\n",
        "# Set tick labels with font size\n",
        "plt.xticks([0.5, 1.5], ['Malignant', 'Benign'], fontsize=20)\n",
        "plt.yticks([0.5, 1.5], ['Malignant', 'Benign'], fontsize=20)\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.title('Confusion Matrix for Breast Cancer in Percentage')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:15:15.207523Z",
          "iopub.status.busy": "2025-07-02T14:15:15.207142Z",
          "iopub.status.idle": "2025-07-02T14:15:15.245058Z",
          "shell.execute_reply": "2025-07-02T14:15:15.243972Z",
          "shell.execute_reply.started": "2025-07-02T14:15:15.207495Z"
        },
        "id": "D3aCYl-FCROu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Generate the classification report as a dictionary\n",
        "report_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Malignant'], output_dict=True)\n",
        "\n",
        "# Convert the report into a pandas DataFrame\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "# Select metrics for classes\n",
        "metrics = report_df.loc[['Benign', 'Malignant'], ['precision', 'recall', 'f1-score', 'support']]\n",
        "\n",
        "# Extract macro and weighted averages\n",
        "macro_avg = report_df.loc['macro avg', ['precision', 'recall', 'f1-score', 'support']]\n",
        "weighted_avg = report_df.loc['weighted avg', ['precision', 'recall', 'f1-score', 'support']]\n",
        "\n",
        "# Append macro and weighted averages to the metrics DataFrame\n",
        "metrics.loc['Macro Avg'] = macro_avg\n",
        "metrics.loc['Weighted Avg'] = weighted_avg\n",
        "\n",
        "# Print the table\n",
        "print(metrics)\n",
        "\n",
        "\n",
        "\n",
        "print(\"retry for 100 epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyglCO3RCROu"
      },
      "source": [
        "From the above, we see that out of **237 malignant** samples (top row), the model correctly identifies **229** as malignant but misclassifies **8** as benign. For the **467** benign samples (bottom row), **459** are correctly predicted as benign, with **8** mistakenly labeled as malignant. Overall, the model does a good job classifying benign cases (low false positives) but still misses some malignant cases (8 false negatives). In a medical context, those false negatives are critical because they represent malignant tumors misdiagnosed as benign. Nonetheless, the overall accuracy is high, indicating strong performance on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:15:20.482079Z",
          "iopub.status.busy": "2025-07-02T14:15:20.481707Z",
          "iopub.status.idle": "2025-07-02T14:15:20.496855Z",
          "shell.execute_reply": "2025-07-02T14:15:20.495743Z",
          "shell.execute_reply.started": "2025-07-02T14:15:20.482048Z"
        },
        "id": "qoXoILR5CROu"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred, target_names = ['Benign (Class 0)','Malignant (Class 1)']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DbDQfSBCROv"
      },
      "source": [
        "- **Precision**: Measures how many of the samples predicted as a certain class (e.g., **Malignant**) actually belong to that class.\n",
        "- **Recall**: (Also known as sensitivity) measures how many of the samples belonging to a certain class (e.g., Malignant) are correctly identified.\n",
        "- **F1 score**: The harmonic mean of precision and recall. It balances both metrics into one number.\n",
        "- **Support**: The number of samples in the dataset belonging to each class.\n",
        "- **Accuracy**: The proportion of all samples (both positive and negative) that were correctly classified.\n",
        "- **Macro avg**: The metric (e.g., precision, recall, F1) independently for each class, and then takes the average. This treats all classes equally.\n",
        "- **Weighted avg**: The metric (e.g., precision, recall, F1) for each class and weights them by the number of samples from that class.​\n",
        "\n",
        "The model does particularly well identifying malignant tumors (high recall of 0.98). This is typically desirable in a medical setting, because missing malignant cases (false negatives) can be more critical. However, the recall for benign cases is lower (0.97), meaning some benign samples are misclassified as malignant. Overall, the model is still quite strong in distinguishing the two classes, as shown by an F1-score above 0.98 for malignant and 0.97 for benign.\n",
        "\n",
        "If we generate an ROC curve by comparing the model's predicted probabilities `y_pred_prob` against the true labels `y_test` at various decision thresholds we can calculate the false positive rate (FPR) and true positive rate (TPR) for each threshold and then plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:15:25.119541Z",
          "iopub.status.busy": "2025-07-02T14:15:25.119103Z",
          "iopub.status.idle": "2025-07-02T14:15:25.562580Z",
          "shell.execute_reply": "2025-07-02T14:15:25.561485Z",
          "shell.execute_reply.started": "2025-07-02T14:15:25.119507Z"
        },
        "id": "toi50BqFCROv"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "area_under_curve = auc(fpr, tpr)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(area_under_curve))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.savefig(\"ROC_Curve.png\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPn9MTiDCROv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:18:39.904934Z",
          "iopub.status.busy": "2025-07-02T14:18:39.904495Z",
          "iopub.status.idle": "2025-07-02T14:20:05.236498Z",
          "shell.execute_reply": "2025-07-02T14:20:05.235293Z",
          "shell.execute_reply.started": "2025-07-02T14:18:39.904897Z"
        },
        "id": "lqcgHVXYCROv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Predict probabilities on train and validation sets\n",
        "y_train_prob = model.predict(train, verbose=1)\n",
        "y_val_prob = model.predict(test, verbose=1)\n",
        "\n",
        "# Get true binary labels\n",
        "y_train_true = train.classes\n",
        "y_val_true = test.classes\n",
        "\n",
        "# Compute ROC curve and AUC for training\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train_true, y_train_prob)\n",
        "auc_train = auc(fpr_train, tpr_train)\n",
        "\n",
        "# Compute ROC curve and AUC for validation\n",
        "fpr_val, tpr_val, _ = roc_curve(y_val_true, y_val_prob)\n",
        "auc_val = auc(fpr_val, tpr_val)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(fpr_train, tpr_train, label=f\"Train AUC = {auc_train:.4f}\", color='blue')\n",
        "plt.plot(fpr_val, tpr_val, label=f\"Validation AUC = {auc_val:.4f}\", color='green')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
        "plt.title(\"ROC Curve for Train vs Validation\", fontsize=16)\n",
        "plt.legend(loc=\"lower right\", fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0mx1pZFCROv"
      },
      "source": [
        "The AUC (Area Under the Curve) of 0.947 shows that our model has strong discriminatory power—it can distinguish malignant from benign cases accurately across a broad range of thresholds. The closer the AUC is to 1.0, the better the model's overall performance.\n",
        "\n",
        "Now, let's sample 25 random test images from the dataset `X_test`. For each image we'll retrieve:\n",
        "\n",
        "1. The predicted class (either 0 or 1) and mapped it to \"Benign\" or \"Malignant\".\n",
        "2. The model's probability for that predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T16:00:05.787738Z",
          "iopub.status.busy": "2025-07-02T16:00:05.784138Z",
          "iopub.status.idle": "2025-07-02T16:00:05.900584Z",
          "shell.execute_reply": "2025-07-02T16:00:05.899472Z",
          "shell.execute_reply.started": "2025-07-02T16:00:05.787519Z"
        },
        "id": "XYp5T5_5CROv"
      },
      "outputs": [],
      "source": [
        "# mapping numeric predictions (0 or 1) to the corresponding string labels.\n",
        "cancer_labels = ['Benign', 'Malignant']\n",
        "\n",
        "# Randomly picking 25 indices from the test set, then retrieving those images.\n",
        "img_indices = np.random.randint(0, len(X_test), size=25)\n",
        "sample_test_images = X_test[img_indices]\n",
        "\n",
        "# Creatinng a list of predicted labels (e.g., \"Benign\" or \"Malignant\") corresponding to each of the chosen images.\n",
        "actual_test_labels = [cancer_labels[int(label)] for label in y_test[img_indices]]\n",
        "\n",
        "max_prediction = np.max(y_pred_prob, axis=1)\n",
        "prediction_probs = np.max(y_pred_prob, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6dvQpleCROv"
      },
      "source": [
        "- `max_prediction`: For each test sample, takes the index of the highest probability. (In a binary classification with one output neuron, this step usually isn't necessary—if you had two output neurons, it picks whichever neuron has the higher probability.)\n",
        "- `prediction_probs`: Grabs the actual probability value of that highest-probability class.\n",
        "\n",
        "We then display each image in a subplot, labeling it with the predicted class and its predicted probability on the x-axis, and (in this snippet) also showing the predicted label again on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T14:15:41.971411Z",
          "iopub.status.busy": "2025-07-02T14:15:41.970980Z",
          "iopub.status.idle": "2025-07-02T14:15:45.073063Z",
          "shell.execute_reply": "2025-07-02T14:15:45.071761Z",
          "shell.execute_reply.started": "2025-07-02T14:15:41.971380Z"
        },
        "id": "uL5nEMKmCROw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for i, (img, pred_idx, prob, true_label) in enumerate(\n",
        "    zip(sample_test_images, max_prediction[img_indices],\n",
        "        prediction_probs[img_indices], actual_test_labels)\n",
        "):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel(f\"{cancer_labels[int(round(pred_idx))]} ({prob:.3f})\")\n",
        "    plt.ylabel(true_label)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cC91EQDCROw"
      },
      "source": [
        "By visually inspecting these images, you can see how confident the model is for each prediction. If \"Benign\" images mostly have predicted probabilities close to 1.0 for \"Benign\", and \"Malignant\" images similarly have high probabilities for \"Malignant\" the model is performing well. In the provided screenshot, most if not all predicted labels and confidence levels appear correct or strongly confident. However, any discrepancies (e.g., a visually \"Malignant\" image labeled as \"Benign\" with a high probability) would signal a misclassification worth investigating."
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6516012,
          "sourceId": 10529195,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}