{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1873742,"sourceType":"datasetVersion","datasetId":1115384}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os  # Interact with the operating system (file handling, paths, etc.)\nimport gc  # Manages garbage collection (memory cleanup)\nimport PIL  # Image processing library (Pillow)\nimport cv2  # OpenCV for image processing and computer vision\nimport uuid  # Generate unique identifiers (used for unique filenames)\nimport shutil  # High-level file operations (copying, moving, deleting)\nimport random  # Generate random numbers, shuffle data, and sampling\nimport glob as gb  # Pattern-based file searching (e.g., \"*.jpg\" for all images)\nimport numpy as np  # Numerical computing with arrays and matrices\nimport pandas as pd  # Data handling with structured DataFrames\nimport tensorflow as tf  # Deep learning framework for building models\nimport matplotlib.pyplot as plt  # Data visualization (plots, graphs, image display)\n\nfrom PIL import Image  # Handling and processing image files\nfrom tqdm import tqdm  # Display progress bars for loops\nfrom scipy.special import gamma  # Gamma function used in probability and statistics\n\nimport keras  # High-level neural network API within TensorFlow\nfrom keras.optimizers import *  # Various optimizers (Adam, SGD, RMSprop, etc.)\nfrom keras.regularizers import l1_l2  # Regularization techniques (L1/L2) to prevent overfitting\nfrom keras.utils import to_categorical  # Converts class labels to one-hot encoding\nfrom keras.callbacks import EarlyStopping  # Stops training early if validation loss increases\nfrom keras.models import Sequential, Model  # Model architectures (Sequential = linear, Model = flexible)\nfrom keras.layers import Dense, Dropout, Input  # Fully connected layers, dropout for regularization, input layer\nfrom keras.layers import GlobalAveragePooling2D  # Replaces fully connected layers to reduce parameters\nfrom keras.callbacks import LearningRateScheduler  # Dynamically adjust learning rate during training\nfrom keras.layers import Conv2D, MaxPool2D, BatchNormalization  # CNN layers for feature extraction & normalization\n\nfrom tensorflow.keras.metrics import *  # Model evaluation metrics (accuracy, precision, recall, etc.)\nfrom tensorflow.keras.callbacks import *  # Various training callbacks (checkpointing, early stopping)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator  # Augments images (rotation, zoom, flip, etc.)\n\nfrom sklearn.model_selection import train_test_split  # Splits data into training and testing sets\nfrom sklearn.utils import shuffle  # Shuffles data to prevent learning order biases","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T14:39:49.440394Z","iopub.execute_input":"2025-06-11T14:39:49.440946Z","iopub.status.idle":"2025-06-11T14:40:07.264019Z","shell.execute_reply.started":"2025-06-11T14:39:49.440904Z","shell.execute_reply":"2025-06-11T14:40:07.262884Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset\"\n\nfor files in os.listdir(path):\n    files_dir = os.path.join(path, files)\n\n    if files == 'jpeg':   # to pass 6774 files \n        pass\n    else:\n        for file in os.listdir(files_dir):\n            print(file)\n\n#Looping through each file/Folder and Ignore the folder named 'jpeg' and loop through the remaining files.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T14:40:07.265368Z","iopub.execute_input":"2025-06-11T14:40:07.266124Z","iopub.status.idle":"2025-06-11T14:40:07.289292Z","shell.execute_reply.started":"2025-06-11T14:40:07.266089Z","shell.execute_reply":"2025-06-11T14:40:07.287792Z"}},"outputs":[{"name":"stdout","text":"dicom_info.csv\nmass_case_description_train_set.csv\ncalc_case_description_train_set.csv\nmeta.csv\ncalc_case_description_test_set.csv\nmass_case_description_test_set.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dicom_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/dicom_info.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.441934Z","iopub.execute_input":"2025-02-24T19:35:31.442263Z","iopub.status.idle":"2025-02-24T19:35:31.664015Z","shell.execute_reply.started":"2025-02-24T19:35:31.442234Z","shell.execute_reply":"2025-02-24T19:35:31.662761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.665692Z","iopub.execute_input":"2025-02-24T19:35:31.666046Z","iopub.status.idle":"2025-02-24T19:35:31.714232Z","shell.execute_reply.started":"2025-02-24T19:35:31.666013Z","shell.execute_reply":"2025-02-24T19:35:31.713114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.715018Z","iopub.execute_input":"2025-02-24T19:35:31.715394Z","iopub.status.idle":"2025-02-24T19:35:31.770272Z","shell.execute_reply.started":"2025-02-24T19:35:31.715354Z","shell.execute_reply":"2025-02-24T19:35:31.769312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.771162Z","iopub.execute_input":"2025-02-24T19:35:31.771458Z","iopub.status.idle":"2025-02-24T19:35:31.801754Z","shell.execute_reply.started":"2025-02-24T19:35:31.771428Z","shell.execute_reply":"2025-02-24T19:35:31.800835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.SeriesDescription.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.802537Z","iopub.execute_input":"2025-02-24T19:35:31.802841Z","iopub.status.idle":"2025-02-24T19:35:31.808485Z","shell.execute_reply.started":"2025-02-24T19:35:31.802814Z","shell.execute_reply":"2025-02-24T19:35:31.807595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.SeriesDescription.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.809346Z","iopub.execute_input":"2025-02-24T19:35:31.809636Z","iopub.status.idle":"2025-02-24T19:35:31.824392Z","shell.execute_reply.started":"2025-02-24T19:35:31.80961Z","shell.execute_reply":"2025-02-24T19:35:31.823525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matr_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv\")\nmatr_df.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.827599Z","iopub.execute_input":"2025-02-24T19:35:31.827899Z","iopub.status.idle":"2025-02-24T19:35:31.878215Z","shell.execute_reply.started":"2025-02-24T19:35:31.827873Z","shell.execute_reply":"2025-02-24T19:35:31.877416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matr_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.879776Z","iopub.execute_input":"2025-02-24T19:35:31.88005Z","iopub.status.idle":"2025-02-24T19:35:31.891712Z","shell.execute_reply.started":"2025-02-24T19:35:31.880023Z","shell.execute_reply":"2025-02-24T19:35:31.890834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matr_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.892665Z","iopub.execute_input":"2025-02-24T19:35:31.892959Z","iopub.status.idle":"2025-02-24T19:35:31.920217Z","shell.execute_reply.started":"2025-02-24T19:35:31.89294Z","shell.execute_reply":"2025-02-24T19:35:31.919418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catr_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_train_set.csv\")\ncatr_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.920972Z","iopub.execute_input":"2025-02-24T19:35:31.92124Z","iopub.status.idle":"2025-02-24T19:35:31.967881Z","shell.execute_reply.started":"2025-02-24T19:35:31.921221Z","shell.execute_reply":"2025-02-24T19:35:31.967038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catr_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.96876Z","iopub.execute_input":"2025-02-24T19:35:31.969038Z","iopub.status.idle":"2025-02-24T19:35:31.981126Z","shell.execute_reply.started":"2025-02-24T19:35:31.969017Z","shell.execute_reply":"2025-02-24T19:35:31.98012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catr_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:31.982112Z","iopub.execute_input":"2025-02-24T19:35:31.982385Z","iopub.status.idle":"2025-02-24T19:35:32.015619Z","shell.execute_reply.started":"2025-02-24T19:35:31.982365Z","shell.execute_reply":"2025-02-24T19:35:32.01482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mate_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv\")\nmate_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.016563Z","iopub.execute_input":"2025-02-24T19:35:32.016858Z","iopub.status.idle":"2025-02-24T19:35:32.041951Z","shell.execute_reply.started":"2025-02-24T19:35:32.016829Z","shell.execute_reply":"2025-02-24T19:35:32.041182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mate_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.042775Z","iopub.execute_input":"2025-02-24T19:35:32.042989Z","iopub.status.idle":"2025-02-24T19:35:32.055467Z","shell.execute_reply.started":"2025-02-24T19:35:32.04297Z","shell.execute_reply":"2025-02-24T19:35:32.054619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mate_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.056356Z","iopub.execute_input":"2025-02-24T19:35:32.056665Z","iopub.status.idle":"2025-02-24T19:35:32.091714Z","shell.execute_reply.started":"2025-02-24T19:35:32.056633Z","shell.execute_reply":"2025-02-24T19:35:32.090961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cate_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_test_set.csv\")\ncate_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.092784Z","iopub.execute_input":"2025-02-24T19:35:32.093029Z","iopub.status.idle":"2025-02-24T19:35:32.115452Z","shell.execute_reply.started":"2025-02-24T19:35:32.093007Z","shell.execute_reply":"2025-02-24T19:35:32.114591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cate_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.116233Z","iopub.execute_input":"2025-02-24T19:35:32.116459Z","iopub.status.idle":"2025-02-24T19:35:32.128581Z","shell.execute_reply.started":"2025-02-24T19:35:32.11644Z","shell.execute_reply":"2025-02-24T19:35:32.127798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cate_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.129422Z","iopub.execute_input":"2025-02-24T19:35:32.129891Z","iopub.status.idle":"2025-02-24T19:35:32.160227Z","shell.execute_reply.started":"2025-02-24T19:35:32.129856Z","shell.execute_reply":"2025-02-24T19:35:32.159282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/meta.csv\")\nmeta_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.16111Z","iopub.execute_input":"2025-02-24T19:35:32.161379Z","iopub.status.idle":"2025-02-24T19:35:32.213475Z","shell.execute_reply.started":"2025-02-24T19:35:32.161355Z","shell.execute_reply":"2025-02-24T19:35:32.212845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.214053Z","iopub.execute_input":"2025-02-24T19:35:32.214245Z","iopub.status.idle":"2025-02-24T19:35:32.22578Z","shell.execute_reply.started":"2025-02-24T19:35:32.214229Z","shell.execute_reply":"2025-02-24T19:35:32.224903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.226497Z","iopub.execute_input":"2025-02-24T19:35:32.226786Z","iopub.status.idle":"2025-02-24T19:35:32.25562Z","shell.execute_reply.started":"2025-02-24T19:35:32.22676Z","shell.execute_reply":"2025-02-24T19:35:32.254917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dicom contains Cropped images, ROI Mask Images, Mammogram Images ","metadata":{}},{"cell_type":"code","source":"cropped_images = dicom_df[dicom_df.SeriesDescription==\"cropped images\"].image_path\ncropped_images.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.256464Z","iopub.execute_input":"2025-02-24T19:35:32.256726Z","iopub.status.idle":"2025-02-24T19:35:32.266545Z","shell.execute_reply.started":"2025-02-24T19:35:32.256696Z","shell.execute_reply":"2025-02-24T19:35:32.265776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_mammogram = dicom_df[dicom_df.SeriesDescription==\"full mammogram images\"].image_path\nfull_mammogram.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.267163Z","iopub.execute_input":"2025-02-24T19:35:32.267439Z","iopub.status.idle":"2025-02-24T19:35:32.284792Z","shell.execute_reply.started":"2025-02-24T19:35:32.267414Z","shell.execute_reply":"2025-02-24T19:35:32.283923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roi_mask = dicom_df[dicom_df.SeriesDescription==\"ROI mask images\"].image_path\nroi_mask.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.285517Z","iopub.execute_input":"2025-02-24T19:35:32.285815Z","iopub.status.idle":"2025-02-24T19:35:32.298032Z","shell.execute_reply.started":"2025-02-24T19:35:32.285784Z","shell.execute_reply":"2025-02-24T19:35:32.297099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # delete dicom_df after finished use it\ndel dicom_df;    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.306151Z","iopub.execute_input":"2025-02-24T19:35:32.306437Z","iopub.status.idle":"2025-02-24T19:35:32.484764Z","shell.execute_reply.started":"2025-02-24T19:35:32.306413Z","shell.execute_reply":"2025-02-24T19:35:32.483958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def replace_path(sample, old_path, new_path):\n    return sample.replace(old_path, new_path, regex=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.48796Z","iopub.execute_input":"2025-02-24T19:35:32.488245Z","iopub.status.idle":"2025-02-24T19:35:32.504403Z","shell.execute_reply.started":"2025-02-24T19:35:32.48822Z","shell.execute_reply":"2025-02-24T19:35:32.503593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_samples(sample, row=40, col=40):\n    plt.figure(figsize=(row, col))\n    for i, file in enumerate(sample[0:10]):\n        cropped_images_show = PIL.Image.open(file)\n        gray_img= cropped_images_show.convert(\"L\")\n        plt.subplot(1,10,i+1)\n        plt.imshow(gray_img, cmap='gray')\n        plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.505231Z","iopub.execute_input":"2025-02-24T19:35:32.505615Z","iopub.status.idle":"2025-02-24T19:35:32.520452Z","shell.execute_reply.started":"2025-02-24T19:35:32.505586Z","shell.execute_reply":"2025-02-24T19:35:32.519658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correct_dir = \"../input/cbis-ddsm-breast-cancer-image-dataset/jpeg\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.52127Z","iopub.execute_input":"2025-02-24T19:35:32.521526Z","iopub.status.idle":"2025-02-24T19:35:32.533955Z","shell.execute_reply.started":"2025-02-24T19:35:32.521502Z","shell.execute_reply":"2025-02-24T19:35:32.533143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images = replace_path(cropped_images, \"CBIS-DDSM/jpeg\", correct_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.534689Z","iopub.execute_input":"2025-02-24T19:35:32.534985Z","iopub.status.idle":"2025-02-24T19:35:32.553932Z","shell.execute_reply.started":"2025-02-24T19:35:32.534961Z","shell.execute_reply":"2025-02-24T19:35:32.553153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_samples(cropped_images, 40,40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:32.554653Z","iopub.execute_input":"2025-02-24T19:35:32.554955Z","iopub.status.idle":"2025-02-24T19:35:33.601587Z","shell.execute_reply.started":"2025-02-24T19:35:32.554929Z","shell.execute_reply":"2025-02-24T19:35:33.600555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_mammogram = replace_path(full_mammogram, \"CBIS-DDSM/jpeg\", correct_dir)\nplot_samples(full_mammogram, 40,40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:33.602561Z","iopub.execute_input":"2025-02-24T19:35:33.602886Z","iopub.status.idle":"2025-02-24T19:35:43.255103Z","shell.execute_reply.started":"2025-02-24T19:35:33.602859Z","shell.execute_reply":"2025-02-24T19:35:43.254183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roi_mask = replace_path(roi_mask, \"CBIS-DDSM/jpeg\", correct_dir)\nplot_samples(roi_mask, 40,40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:43.256094Z","iopub.execute_input":"2025-02-24T19:35:43.256407Z","iopub.status.idle":"2025-02-24T19:35:51.718473Z","shell.execute_reply.started":"2025-02-24T19:35:43.256378Z","shell.execute_reply":"2025-02-24T19:35:51.717582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_file_name(data, new_dict):\n    for dicom in data:\n        key = dicom.split('/')[4]\n        new_dict[key] = dicom\n    print(f\"the length of dataset ==> {len(new_dict.keys())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:51.719341Z","iopub.execute_input":"2025-02-24T19:35:51.71961Z","iopub.status.idle":"2025-02-24T19:35:51.724039Z","shell.execute_reply.started":"2025-02-24T19:35:51.719588Z","shell.execute_reply":"2025-02-24T19:35:51.723062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images_dict = dict()\nfull_mammo_dict = dict()\nroi_img_dict = dict()\n\nget_image_file_name(cropped_images, cropped_images_dict)\nget_image_file_name(full_mammogram, full_mammo_dict)\nget_image_file_name(roi_mask, roi_img_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:51.724702Z","iopub.execute_input":"2025-02-24T19:35:51.724945Z","iopub.status.idle":"2025-02-24T19:35:51.743462Z","shell.execute_reply.started":"2025-02-24T19:35:51.724926Z","shell.execute_reply":"2025-02-24T19:35:51.742835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del cropped_images, full_mammogram, roi_mask;    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:51.744278Z","iopub.execute_input":"2025-02-24T19:35:51.744569Z","iopub.status.idle":"2025-02-24T19:35:51.961874Z","shell.execute_reply.started":"2025-02-24T19:35:51.744541Z","shell.execute_reply":"2025-02-24T19:35:51.960954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fix_image_path(data):\n    for indx, image in enumerate(data.values):\n        img_name = image[11].split('/')[2]\n        if img_name in full_mammo_dict:\n            data.iloc[indx, 11] = full_mammo_dict[img_name]\n        else:\n            data.iloc[indx, 11] = None\n        img_name = image[12].split('/')[2]\n        if img_name in cropped_images_dict:\n            data.iloc[indx, 12] = cropped_images_dict[img_name]\n        else:\n            data.iloc[indx, 11] = None\n        img_name = image[13].split('/')[2]\n        if img_name in roi_img_dict:\n            data.iloc[indx, 13] = roi_img_dict[img_name]\n        else:\n            data.iloc[indx, 13] = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:51.96266Z","iopub.execute_input":"2025-02-24T19:35:51.962961Z","iopub.status.idle":"2025-02-24T19:35:51.974808Z","shell.execute_reply.started":"2025-02-24T19:35:51.96294Z","shell.execute_reply":"2025-02-24T19:35:51.974065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(matr_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:51.975537Z","iopub.execute_input":"2025-02-24T19:35:51.975828Z","iopub.status.idle":"2025-02-24T19:35:52.52232Z","shell.execute_reply.started":"2025-02-24T19:35:51.975801Z","shell.execute_reply":"2025-02-24T19:35:52.521767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matr_df = matr_df.rename(columns={'left or right breast': 'left_or_right_breast',\n                                        'image view': 'image_view',\n                                        'abnormality id': 'abnormality_id',\n                                        'abnormality type': 'abnormality_type',\n                                        'mass shape': 'mass_shape',\n                                        'mass margins': 'mass_margins',\n                                        'image file path': 'image_file_path',\n                                        'cropped image file path': 'cropped_image_file_path',\n                                        'ROI mask file path': 'ROI_mask_file_path'})\nmatr_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:52.523014Z","iopub.execute_input":"2025-02-24T19:35:52.523289Z","iopub.status.idle":"2025-02-24T19:35:52.535831Z","shell.execute_reply.started":"2025-02-24T19:35:52.523268Z","shell.execute_reply":"2025-02-24T19:35:52.534991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matr_df.pathology.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:52.53668Z","iopub.execute_input":"2025-02-24T19:35:52.53693Z","iopub.status.idle":"2025-02-24T19:35:52.549004Z","shell.execute_reply.started":"2025-02-24T19:35:52.536912Z","shell.execute_reply":"2025-02-24T19:35:52.548268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(mate_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:52.549679Z","iopub.execute_input":"2025-02-24T19:35:52.549936Z","iopub.status.idle":"2025-02-24T19:35:52.716322Z","shell.execute_reply.started":"2025-02-24T19:35:52.549918Z","shell.execute_reply":"2025-02-24T19:35:52.715794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mate_df= mate_df.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\nmate_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:52.717011Z","iopub.execute_input":"2025-02-24T19:35:52.717194Z","iopub.status.idle":"2025-02-24T19:35:52.729276Z","shell.execute_reply.started":"2025-02-24T19:35:52.717178Z","shell.execute_reply":"2025-02-24T19:35:52.728584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(catr_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:52.730055Z","iopub.execute_input":"2025-02-24T19:35:52.730321Z","iopub.status.idle":"2025-02-24T19:35:53.359155Z","shell.execute_reply.started":"2025-02-24T19:35:52.730301Z","shell.execute_reply":"2025-02-24T19:35:53.358583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catr_df= catr_df.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\ncatr_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:53.359829Z","iopub.execute_input":"2025-02-24T19:35:53.360045Z","iopub.status.idle":"2025-02-24T19:35:53.372711Z","shell.execute_reply.started":"2025-02-24T19:35:53.360027Z","shell.execute_reply":"2025-02-24T19:35:53.372021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(cate_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:53.373575Z","iopub.execute_input":"2025-02-24T19:35:53.373902Z","iopub.status.idle":"2025-02-24T19:35:53.517082Z","shell.execute_reply.started":"2025-02-24T19:35:53.37387Z","shell.execute_reply":"2025-02-24T19:35:53.516403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cate_df = cate_df.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\ncate_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:53.517884Z","iopub.execute_input":"2025-02-24T19:35:53.518082Z","iopub.status.idle":"2025-02-24T19:35:53.530007Z","shell.execute_reply.started":"2025-02-24T19:35:53.518065Z","shell.execute_reply":"2025-02-24T19:35:53.529322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_images(dataset, column, number):\n    fig, axes = plt.subplots(1, number, figsize=(15, 5))\n    for index, (i, row) in enumerate(dataset.head(number).iterrows()):\n        image_path = row[column]\n        if image_path is None or not os.path.exists(image_path):\n            continue\n        \n        image = cv2.imread(image_path)\n        \n        if image is None:\n            continue\n        if len(image.shape) == 3 and image.shape[2] == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        ax = axes[index]\n        ax.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n        ax.set_title(f\"{row['pathology']}\")\n        ax.axis('off')\n        print(np.array(image).shape)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:53.530827Z","iopub.execute_input":"2025-02-24T19:35:53.531073Z","iopub.status.idle":"2025-02-24T19:35:53.543286Z","shell.execute_reply.started":"2025-02-24T19:35:53.531042Z","shell.execute_reply":"2025-02-24T19:35:53.542632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(matr_df, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(matr_df, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(matr_df, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:35:53.544073Z","iopub.execute_input":"2025-02-24T19:35:53.544287Z","iopub.status.idle":"2025-02-24T19:36:10.827647Z","shell.execute_reply.started":"2025-02-24T19:35:53.544256Z","shell.execute_reply":"2025-02-24T19:36:10.82684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(mate_df, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(mate_df, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(mate_df, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:10.82853Z","iopub.execute_input":"2025-02-24T19:36:10.828901Z","iopub.status.idle":"2025-02-24T19:36:26.708791Z","shell.execute_reply.started":"2025-02-24T19:36:10.828867Z","shell.execute_reply":"2025-02-24T19:36:26.708038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(catr_df, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(catr_df, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(catr_df, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:26.709517Z","iopub.execute_input":"2025-02-24T19:36:26.709759Z","iopub.status.idle":"2025-02-24T19:36:43.966071Z","shell.execute_reply.started":"2025-02-24T19:36:26.709715Z","shell.execute_reply":"2025-02-24T19:36:43.965175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(cate_df, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(cate_df, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(cate_df, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:43.966974Z","iopub.execute_input":"2025-02-24T19:36:43.967274Z","iopub.status.idle":"2025-02-24T19:36:45.961553Z","shell.execute_reply.started":"2025-02-24T19:36:43.967241Z","shell.execute_reply":"2025-02-24T19:36:45.960273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_dataset = pd.concat([catr_df, cate_df], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:45.962424Z","iopub.execute_input":"2025-02-24T19:36:45.962727Z","iopub.status.idle":"2025-02-24T19:36:45.967789Z","shell.execute_reply.started":"2025-02-24T19:36:45.962695Z","shell.execute_reply":"2025-02-24T19:36:45.966882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del matr_df,mate_df, catr_df, cate_df;    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:45.968557Z","iopub.execute_input":"2025-02-24T19:36:45.968856Z","iopub.status.idle":"2025-02-24T19:36:46.169183Z","shell.execute_reply.started":"2025-02-24T19:36:45.968823Z","shell.execute_reply":"2025-02-24T19:36:46.168461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:46.170026Z","iopub.execute_input":"2025-02-24T19:36:46.170285Z","iopub.status.idle":"2025-02-24T19:36:46.181811Z","shell.execute_reply.started":"2025-02-24T19:36:46.170265Z","shell.execute_reply":"2025-02-24T19:36:46.181111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_size = (224, 224, 3) #The height and width of the image (in pixels).The number of color channels (RGB → Red, Green, Blue).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:46.182477Z","iopub.execute_input":"2025-02-24T19:36:46.182707Z","iopub.status.idle":"2025-02-24T19:36:46.193754Z","shell.execute_reply.started":"2025-02-24T19:36:46.182688Z","shell.execute_reply":"2025-02-24T19:36:46.19308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# maps pathology labels to numerical values, and extracts valid image paths and their corresponding labels.\ncalc_dataset['labels'] = calc_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n\ncalc_images = np.array(calc_dataset[calc_dataset[\"image_file_path\"].notna()][\"image_file_path\"].tolist())\ncalc_labels = np.array(calc_dataset[calc_dataset[\"image_file_path\"].notna()][\"labels\"].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:46.194435Z","iopub.execute_input":"2025-02-24T19:36:46.194707Z","iopub.status.idle":"2025-02-24T19:36:46.212391Z","shell.execute_reply.started":"2025-02-24T19:36:46.194682Z","shell.execute_reply":"2025-02-24T19:36:46.211648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_labels_series = pd.Series(calc_labels)\n\n# Count the occurrences of each class\nlabel_counts = full_labels_series.value_counts()\n\n# Assuming 0 = benign and 1 = malignant\nbenign_count = label_counts.get(0, 0)\nmalignant_count = label_counts.get(1, 0)\n\nprint(f\"Benign images: {benign_count}\")\nprint(f\"Malignant images: {malignant_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:46.213216Z","iopub.execute_input":"2025-02-24T19:36:46.213545Z","iopub.status.idle":"2025-02-24T19:36:46.221322Z","shell.execute_reply.started":"2025-02-24T19:36:46.213515Z","shell.execute_reply":"2025-02-24T19:36:46.220492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def images_count():\n    zero_class_count = len(os.listdir(\"/kaggle/working/merged_images/0\"))\n    one_class_count  = len(os.listdir(\"/kaggle/working/merged_images/1\"))\n\n    print(f\"Number of images in class 0: {zero_class_count}\")\n    print(f\"Number of images in class 1: {one_class_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:46.222142Z","iopub.execute_input":"2025-02-24T19:36:46.222403Z","iopub.status.idle":"2025-02-24T19:36:46.231444Z","shell.execute_reply.started":"2025-02-24T19:36:46.222384Z","shell.execute_reply":"2025-02-24T19:36:46.2308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#data augmentation techniques to an image using TensorFlow's tf.image module. Data augmentation artificially increases dataset diversity by applying random transformations, making the model more robust and generalized.\ndef augment_image(image):\n    # Apply data augmentation using tf.image functions\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, max_delta=0.3)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n    return image\n\n# Function to resize image to (224, 224, 3)\ndef resize_image(image_tensor):\n    return tf.image.resize(image_tensor, [224, 224])\n\n# Function to balance classes by augmenting images\ndef copy_images_with_unique_filenames(images, labels, source, destination, target_count=None):\n    \n    benign_images = 0\n    malignant_images = 0\n    skipped_images = []\n\n    # Create the destination subfolders '0' and '1'\n    category_dest_dir_zero = os.path.join(destination, '0')\n    os.makedirs(category_dest_dir_zero, exist_ok=True)\n\n    category_dest_dir_one = os.path.join(destination, '1')\n    os.makedirs(category_dest_dir_one, exist_ok=True)\n\n    benign_images_list = []\n    malignant_images_list = []\n\n    for i, (image, label) in enumerate(zip(images, labels)):\n        if os.path.exists(image):\n            try:\n                # Generate a unique filename\n                filename = os.path.basename(image)\n                unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n        \n                # Open the image using PIL\n                with Image.open(image) as img:\n                    # Convert the image to RGB mode (for saving as JPEG)\n                    img = img.convert('RGB')\n                    # Augment the image (convert it to a Tensor first)\n                    img_tensor = tf.convert_to_tensor(img)\n                    # Resize the image to (224, 224, 3)\n                    resized_img_tensor = resize_image(img_tensor)\n                    augmented_image_tensor = augment_image(resized_img_tensor)\n                    # Convert Tensor back to PIL image for saving\n                    augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n\n                    if label == 0:\n                        benign_images_list.append(unique_filename)\n                        dest_path = os.path.join(category_dest_dir_zero, unique_filename)\n                        augmented_image.save(dest_path, 'JPEG')\n                        benign_images += 1\n\n                    elif label == 1:\n                        malignant_images_list.append(unique_filename)\n                        dest_path = os.path.join(category_dest_dir_one, unique_filename)\n                        augmented_image.save(dest_path, 'JPEG')\n                        malignant_images += 1\n\n            except Exception as e:\n                print(f\"Error copying image {image}: {e}\")\n                skipped_images.append(image)\n        else:\n            print(f\"Image not found: {image}\")\n            skipped_images.append(image)\n\n    # If balancing is needed, duplicate/augment images from the smaller class\n    benign_count = len(benign_images_list)\n    malignant_count = len(malignant_images_list)\n\n    if benign_count < malignant_count:\n        augment_and_save_images(benign_images_list, category_dest_dir_zero, malignant_count - benign_count)\n\n    elif malignant_count < benign_count:\n        augment_and_save_images(malignant_images_list, category_dest_dir_one, benign_count - malignant_count)\n    print(\"data balancing\")\n    images_count()\n    augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count)\n    augment_and_save_images(malignant_images_list, category_dest_dir_one, target_count)\n    print(\"data augmentation\")\n    images_count()\n    \n    print(f\"\\nCopying complete.\")\n    print(f\"Benign images copied (label 0): {benign_images}\")\n    print(f\"Benign count (label 0): {benign_count}\")\n    print(f\"Malignant images copied (label 1): {malignant_images}\")\n    print(f\"Malignant count (label 1): {malignant_count}\")\n    print(f\"Total skipped images: {len(skipped_images)}\")\n    if skipped_images:\n        print(\"Skipped images:\")\n        for img in skipped_images:\n            print(img)\n            \n    del skipped_images, benign_images_list, malignant_images_list\n    gc.collect()\n\n# Function to augment and save images to balance the dataset\ndef augment_and_save_images(images_list, destination_dir, num_augments):\n    for i in range(num_augments):\n        img_name = random.choice(images_list)\n        abs_path = os.path.join(destination_dir, img_name)\n\n        try:\n            with Image.open(abs_path) as img:\n                img = img.convert('RGB')\n                # Augment the image\n                img_tensor = tf.convert_to_tensor(img)\n                # Resize the image\n                augmented_image_tensor = augment_image(img_tensor)\n                # Convert Tensor back to PIL image for saving\n                augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n                # Remove the original extension from img_name 1-285.jpg --> 1-285\n                img_name_without_ext = os.path.splitext(img_name)[0]\n                # Save augmented image with a unique name\n                augmented_image.save(os.path.join(destination_dir, img_name_without_ext + f'_aug{i}.jpg'), 'JPEG')\n        except Exception as e:\n            print(f\"Error augmenting image {abs_path}: {e}\")\n\n# Example usage\nsource_dir = \"/kaggle/input/mias-mammography/all-mias\"\ndestination_dir = \"/kaggle/working/merged_images\"\n\n# target_count=0 meaning no Augmentation, There's just Data-Balance\ntarget_count = int((len(calc_labels) * 5) // 2)\ncopy_images_with_unique_filenames(calc_images, calc_labels, source_dir, destination_dir, target_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:36:46.232432Z","iopub.execute_input":"2025-02-24T19:36:46.232762Z","iopub.status.idle":"2025-02-24T19:43:51.061669Z","shell.execute_reply.started":"2025-02-24T19:36:46.23271Z","shell.execute_reply":"2025-02-24T19:43:51.060816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = '/kaggle/working/merged_images'  # Update with your dataset path\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',\n    label_mode='categorical',\n    # image_size=(224, 224),\n    image_size=(224, 224),\n    seed=30,\n    shuffle=True,\n    batch_size=13  # Set your desired batch size\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:43:51.062553Z","iopub.execute_input":"2025-02-24T19:43:51.06283Z","iopub.status.idle":"2025-02-24T19:43:53.097981Z","shell.execute_reply.started":"2025-02-24T19:43:51.062797Z","shell.execute_reply":"2025-02-24T19:43:53.097043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, labels in full_dataset.take(1):  # Take the first batch\n    print(\"Shape of Images:\", images.shape)  # Should be (batch_size, 224, 224, 3)\n    print(\"Shape of Labels:\", labels.shape)  # Should be (batch_size, number_of_classes)\n    \n    # Print first 5 label values\n    print(\"\\nFirst 10 Labels (One-Hot Encoded):\\n\", labels.numpy()[:10])\n    \n    # Display some images with their labels\n    plt.figure(figsize=(10, 5))\n    for i in range(min(10, len(images))): \n        plt.subplot(1, 10, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert tensor to image format\n        plt.axis(\"off\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:43:53.098912Z","iopub.execute_input":"2025-02-24T19:43:53.099223Z","iopub.status.idle":"2025-02-24T19:43:53.438628Z","shell.execute_reply.started":"2025-02-24T19:43:53.099194Z","shell.execute_reply":"2025-02-24T19:43:53.43761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\n# Step 3: Split the dataset into train, validation, and test sets\ntrain_size = int(0.8* total_samples)                 # 80% for training\ntest_size = total_samples - train_size                # 20% for testing\n\n# Create train, validation, and test datasets\ntrain_dataset       = full_dataset.take(train_size)\ntest_dataset        = full_dataset.skip(train_size)\n\n# Prefetch to Improve Performance\ntrain_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# Print the number of samples in each dataset\nprint(f\"Train samples:      {train_size}     batches(8) ==> {train_size*13}\")\nprint(f\"Test samples:       {test_size}      batches(8) ==> {test_size*13}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:43:53.43979Z","iopub.execute_input":"2025-02-24T19:43:53.440034Z","iopub.status.idle":"2025-02-24T19:43:53.454195Z","shell.execute_reply.started":"2025-02-24T19:43:53.440014Z","shell.execute_reply":"2025-02-24T19:43:53.453548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_missing_samples(dataset, dataset_name):\n    missing_count = 0\n    for batch, labels in dataset:\n        batch_nan = np.any(np.isnan(batch.numpy()))  # Check if any image has NaN values\n        label_nan = np.any(np.isnan(labels.numpy()))  # Check if any label is NaN\n        \n        if batch_nan or label_nan:\n            missing_count += 1\n    \n    print(f\"{dataset_name}: Found {missing_count} batches with missing values\")\n\n# Check both training and test datasets\ncheck_missing_samples(train_dataset, \"Train Dataset\")\ncheck_missing_samples(test_dataset, \"Test Dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:43:53.455023Z","iopub.execute_input":"2025-02-24T19:43:53.455319Z","iopub.status.idle":"2025-02-24T19:43:58.49231Z","shell.execute_reply.started":"2025-02-24T19:43:53.455289Z","shell.execute_reply":"2025-02-24T19:43:58.491307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#applies a negative transformation to medical images (benign & malignant cases) and saves them in a new directory.\n#The negative transformation helps enhance contrast and highlight subtle patterns,\n\n\n# Define the data directory and the output directory\ndata_dir = '/kaggle/working/merged_images'\nNeg_dir = '/kaggle/working/Negative_Images'\n\n# Create the output directory if it doesn't exist\nos.makedirs(Neg_dir, exist_ok=True)\n\n# Define the subdirectories for benign and malignant images\nsubdirs = ['0', '1']  # 0 for Benign, 1 for Malignant\n\nfor subdir in subdirs:\n    # Create a corresponding subdirectory in the output directory\n    os.makedirs(os.path.join(Neg_dir, subdir), exist_ok=True)\n    \n    # Define the path to the current subdirectory\n    current_dir = os.path.join(data_dir, subdir)\n    \n    # Loop through all images in the current subdirectory\n    for filename in os.listdir(current_dir):\n        # Construct the full file path\n        file_path = os.path.join(current_dir, filename)\n        \n        # Read the image\n        image = cv2.imread(file_path)\n        \n        # Check if the image was loaded successfully\n        if image is not None:\n            # Convert to grayscale (if necessary)\n            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n            # Apply negative transformation\n            negative_image = 255 - gray_image\n            \n            # Save the negative image in the corresponding output directory\n            output_file_path = os.path.join(Neg_dir, subdir, filename)\n            cv2.imwrite(output_file_path, negative_image)\n        else:\n            print(f\"Warning: Could not read image {file_path}\")\n\nprint(\"Negative transformation applied and images saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:43:58.493245Z","iopub.execute_input":"2025-02-24T19:43:58.493504Z","iopub.status.idle":"2025-02-24T19:44:03.235721Z","shell.execute_reply.started":"2025-02-24T19:43:58.49346Z","shell.execute_reply":"2025-02-24T19:44:03.235004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Verifies that negative images were correctly processed\n#Ensures the images are properly stored in their respective directories (0 = Benign, 1 = Malignant)\n#Helps identify errors or incorrect image transformations\n#Useful for sanity checks before training ResNet50 on negative images\n\nimport random\n\n\n# Function to display one random sample image from each category\ndef display_random_samples(output_dir, subdirs):\n    plt.figure(figsize=(10, 5))\n    \n    for subdir in subdirs:\n        current_dir = os.path.join(output_dir, subdir)\n        sample_files = os.listdir(current_dir)\n        \n        # Select a random file from the list\n        if sample_files:  # Check if there are any files\n            random_file = random.choice(sample_files)\n            file_path = os.path.join(current_dir, random_file)\n            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale for display\n            \n            plt.subplot(1, 2, int(subdir) + 1)\n            plt.imshow(image, cmap='gray')\n            plt.title('Benign' if subdir == '0' else 'Malignant')\n            plt.axis('off')\n    \n    plt.suptitle('Random Negative Images')\n    plt.show()\n\n# Display one random sample from the output directory\ndisplay_random_samples(Neg_dir, subdirs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:44:03.236511Z","iopub.execute_input":"2025-02-24T19:44:03.236791Z","iopub.status.idle":"2025-02-24T19:44:03.444169Z","shell.execute_reply.started":"2025-02-24T19:44:03.236758Z","shell.execute_reply":"2025-02-24T19:44:03.443323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikit-image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:44:03.44517Z","iopub.execute_input":"2025-02-24T19:44:03.445474Z","iopub.status.idle":"2025-02-24T19:44:07.572916Z","shell.execute_reply.started":"2025-02-24T19:44:03.445445Z","shell.execute_reply":"2025-02-24T19:44:07.571716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Enhances contrast in medical images (especially mammograms).\n#Makes tumors, tissues, and patterns more visible.\n#Prepares preprocessed images for ResNet50 training.\n#Ensures equal brightness across different regions (compared to standard histogram equalization).\n\nfrom skimage import exposure\n\n# Define the data directory and the output directory\ndata_dir = '/kaggle/working/merged_images'\nAHE_dir = '/kaggle/working/AHistogram_Images'\n\n# Create the output directory if it doesn't exist\nos.makedirs(AHE_dir, exist_ok=True)\n\n# Define the subdirectories for benign and malignant images\nsubdirs = ['0', '1']  # 0 for Benign, 1 for Malignant\n\n# Function to apply Adaptive Histogram Equalization and save images\ndef apply_adaptive_histogram_equalization(data_dir, output_dir):\n    for subdir in subdirs:\n        # Create a corresponding subdirectory in the output directory\n        os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)\n        \n        # Define the path to the current subdirectory\n        current_dir = os.path.join(data_dir, subdir)\n        \n        # Loop through all images in the current subdirectory\n        for filename in os.listdir(current_dir):\n            # Construct the full file path\n            file_path = os.path.join(current_dir, filename)\n            \n            # Read the image\n            image = cv2.imread(file_path)\n            \n            # Check if the image was loaded successfully\n            if image is not None:\n                # Convert to grayscale\n                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n                \n                # Apply Adaptive Histogram Equalization\n                image_adapteq = exposure.equalize_adapthist(gray_image, clip_limit=0.03)\n                \n                # Convert the image back to uint8 format (0-255)\n                image_adapteq = (image_adapteq * 255).astype(np.uint8)\n                \n                # Save the equalized image in the corresponding output directory\n                output_file_path = os.path.join(output_dir, subdir, filename)\n                cv2.imwrite(output_file_path, image_adapteq)\n            else:\n                print(f\"Warning: Could not read image {file_path}\")\n\n# Call the function to apply the transformation\napply_adaptive_histogram_equalization(data_dir, AHE_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:44:07.57429Z","iopub.execute_input":"2025-02-24T19:44:07.574687Z","iopub.status.idle":"2025-02-24T19:45:17.025676Z","shell.execute_reply.started":"2025-02-24T19:44:07.574651Z","shell.execute_reply":"2025-02-24T19:45:17.024984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to display one random sample image from each category\ndef display_random_samples(output_dir, subdirs):\n    plt.figure(figsize=(10, 5))\n    \n    for subdir in subdirs:\n        current_dir = os.path.join(output_dir, subdir)\n        sample_files = os.listdir(current_dir)\n        \n        # Select a random file from the list\n        if sample_files:  # Check if there are any files\n            random_file = random.choice(sample_files)\n            file_path = os.path.join(current_dir, random_file)\n            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale for display\n            \n            plt.subplot(1, 2, int(subdir) + 1)\n            plt.imshow(image, cmap='gray')\n            plt.title('Benign' if subdir == '0' else 'Malignant')\n            plt.axis('off')\n    \n    plt.suptitle('Random Adaptive Histogram Equalization Images')\n    plt.show()\n\n\n# Display one random sample from the output directory\ndisplay_random_samples('/kaggle/working/AHistogram_Images', subdirs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:45:17.026561Z","iopub.execute_input":"2025-02-24T19:45:17.026941Z","iopub.status.idle":"2025-02-24T19:45:17.254816Z","shell.execute_reply.started":"2025-02-24T19:45:17.02689Z","shell.execute_reply":"2025-02-24T19:45:17.254025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_images():\n    # Get the image names from the merged images directory\n    image_names = [f for f in os.listdir('/kaggle/working/merged_images/1') if f.endswith(('.jpg', '.png'))][:5]\n\n    # Create subplots for 5 images and 3 columns (original, Negative , Adaptive histogram equalized)\n    fig, axs = plt.subplots(5, 3, figsize=(10, 15))\n\n    for i, image_name in enumerate(image_names):\n        # Get the full paths to the images\n        image_path1 = os.path.join('/kaggle/working/merged_images/1', image_name)  # Original Image\n        image_path2 = os.path.join('/kaggle/working/Negative_Images/1', image_name)  # Negative Image\n        image_path3 = os.path.join('/kaggle/working/AHistogram_Images/1', image_name)  # Adaptive Histogram Equalized Image\n\n        # Open the images\n        image1 = Image.open(image_path1).convert('L')  # Convert to grayscale for original\n        image2 = Image.open(image_path2)\n        image3 = Image.open(image_path3)\n\n        # Display the images in the subplots\n        axs[i, 0].imshow(image1, cmap='gray')\n        axs[i, 0].axis('off')\n        axs[i, 0].set_title('Original Image')\n\n        axs[i, 1].imshow(image2, cmap='gray')\n        axs[i, 1].axis('off')\n        axs[i, 1].set_title('Negative Image')\n\n        axs[i, 2].imshow(image3, cmap='gray')\n        axs[i, 2].axis('off')\n        axs[i, 2].set_title('Adaptive Histogram Equalized Image')\n\n    plt.tight_layout()\n    plt.show()\n\ndisplay_images()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:45:17.255749Z","iopub.execute_input":"2025-02-24T19:45:17.256118Z","iopub.status.idle":"2025-02-24T19:45:18.541183Z","shell.execute_reply.started":"2025-02-24T19:45:17.256076Z","shell.execute_reply":"2025-02-24T19:45:18.540245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage import feature\n\n# Define the data directory and the output directory\ndata_dir = '/kaggle/working/merged_images'\noutput_dir = '/kaggle/working/Hog_Images'\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Define the subdirectories for benign and malignant images\nsubdirs = ['0', '1']  # 0 for Benign, 1 for Malignant\n\ndef apply_hog_and_save(data_dir, output_dir):\n    # Iterate through each subdirectory\n    for subdir in subdirs:\n        # Create a corresponding subdirectory in the output directory\n        os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)\n        \n        # Define the path to the current subdirectory\n        current_dir = os.path.join(data_dir, subdir)\n        \n        # Get the list of image files in the current subdirectory\n        image_files = os.listdir(current_dir)\n        \n        # Process each image file\n        for filename in image_files:\n            # Prepare the full path for the image\n            file_path = os.path.join(current_dir, filename)\n            output_path = os.path.join(output_dir, subdir, filename)\n            \n            # Read the image\n            image = cv2.imread(file_path)\n            \n            # Check if the image was loaded successfully\n            if image is not None:\n                # Convert to grayscale\n                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n                \n                # Compute HOG features\n                hog_features, hog_image = feature.hog(\n                    gray_image,\n                    orientations=9,\n                    pixels_per_cell=(8, 8),\n                    cells_per_block=(2, 2),\n                    visualize=True\n                )\n                \n                # Scale the HOG image for better visualization\n                hog_image = (hog_image * 255).astype(np.uint8)\n                \n                # Save the HOG image\n                cv2.imwrite(output_path, hog_image)\n            else:\n                print(f\"Warning: Could not read image {file_path}\")\n\n# Call the function to apply HOG and save images\napply_hog_and_save(data_dir, output_dir)\n\nprint(\"HOG features extracted and images saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:45:18.542118Z","iopub.execute_input":"2025-02-24T19:45:18.542388Z","iopub.status.idle":"2025-02-24T20:13:56.887841Z","shell.execute_reply.started":"2025-02-24T19:45:18.54236Z","shell.execute_reply":"2025-02-24T20:13:56.886993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n\n# Function to display one random sample image from each category\ndef display_random_samplesHog(output_dir, subdirs):\n    plt.figure(figsize=(10, 5))\n    \n    for subdir in subdirs:\n        current_dir = os.path.join(output_dir, subdir)\n        sample_files = os.listdir(current_dir)\n        \n        # Select a random file from the list\n        if sample_files:  # Check if there are any files\n            random_file = random.choice(sample_files)\n            file_path = os.path.join(current_dir, random_file)\n            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale for display\n            \n            plt.subplot(1, 2, int(subdir) + 1)\n            plt.imshow(image, cmap='gray')\n            plt.title('Benign' if subdir == '0' else 'Malignant')\n            plt.axis('off')\n    \n    plt.suptitle('Random HOG Original  Images')\n    plt.show()\n\nsubdirs = ['0', '1']\n\n# Display one random sample from the output directory\ndisplay_random_samplesHog('/kaggle/working/Hog_Images', subdirs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:13:56.888828Z","iopub.execute_input":"2025-02-24T20:13:56.890235Z","iopub.status.idle":"2025-02-24T20:13:57.126467Z","shell.execute_reply.started":"2025-02-24T20:13:56.89021Z","shell.execute_reply":"2025-02-24T20:13:57.125569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the data directory and the output directory\ndata_dir = '/kaggle/working/Negative_Images'\noutput_dir = '/kaggle/working/HogNeg_Images'\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Call the function to apply HOG and save images\napply_hog_and_save(data_dir, output_dir)\n\nprint(\"HOG features extracted and images saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:13:57.127643Z","iopub.execute_input":"2025-02-24T20:13:57.127994Z","iopub.status.idle":"2025-02-24T20:42:11.471718Z","shell.execute_reply.started":"2025-02-24T20:13:57.127968Z","shell.execute_reply":"2025-02-24T20:42:11.470836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n\n# Function to display one random sample image from each category\ndef display_random_samplesHogNeg(output_dir, subdirs):\n    plt.figure(figsize=(10, 5))\n    \n    for subdir in subdirs:\n        current_dir = os.path.join(output_dir, subdir)\n        sample_files = os.listdir(current_dir)\n        \n        # Select a random file from the list\n        if sample_files:  # Check if there are any files\n            random_file = random.choice(sample_files)\n            file_path = os.path.join(current_dir, random_file)\n            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale for display\n            \n            plt.subplot(1, 2, int(subdir) + 1)\n            plt.imshow(image, cmap='gray')\n            plt.title('Benign' if subdir == '0' else 'Malignant')\n            plt.axis('off')\n    \n    plt.suptitle('Random HOG Negative Images')\n    plt.show()\n\nsubdirs = ['0', '1']\n\n# Display one random sample from the output directory\ndisplay_random_samplesHogNeg('/kaggle/working/HogNeg_Images', subdirs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:42:11.47277Z","iopub.execute_input":"2025-02-24T20:42:11.473102Z","iopub.status.idle":"2025-02-24T20:42:11.687304Z","shell.execute_reply.started":"2025-02-24T20:42:11.473069Z","shell.execute_reply":"2025-02-24T20:42:11.686508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the data directory and the output directory\ndata_dir = '/kaggle/working/AHistogram_Images'\noutput_dir = '/kaggle/working/HogAHE_Images'\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Call the function to apply HOG and save images\napply_hog_and_save(data_dir, output_dir)\n\nprint(\"HOG features extracted and images saved successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:42:11.68814Z","iopub.execute_input":"2025-02-24T20:42:11.688409Z","iopub.status.idle":"2025-02-24T21:10:38.745387Z","shell.execute_reply.started":"2025-02-24T20:42:11.688384Z","shell.execute_reply":"2025-02-24T21:10:38.744437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n\n# Function to display one random sample image from each category\ndef display_random_samplesHogNeg(output_dir, subdirs):\n    plt.figure(figsize=(10, 5))\n    \n    for subdir in subdirs:\n        current_dir = os.path.join(output_dir, subdir)\n        sample_files = os.listdir(current_dir)\n        \n        # Select a random file from the list\n        if sample_files:  # Check if there are any files\n            random_file = random.choice(sample_files)\n            file_path = os.path.join(current_dir, random_file)\n            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale for display\n            \n            plt.subplot(1, 2, int(subdir) + 1)\n            plt.imshow(image, cmap='gray')\n            plt.title('Benign' if subdir == '0' else 'Malignant')\n            plt.axis('off')\n    \n    plt.suptitle('Random HOG AHE Images')\n    plt.show()\n\nsubdirs = ['0', '1']\n\n# Display one random sample from the output directory\ndisplay_random_samplesHogNeg('/kaggle/working/HogAHE_Images', subdirs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:10:38.746491Z","iopub.execute_input":"2025-02-24T21:10:38.746818Z","iopub.status.idle":"2025-02-24T21:10:38.97476Z","shell.execute_reply.started":"2025-02-24T21:10:38.746789Z","shell.execute_reply":"2025-02-24T21:10:38.973949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Define the data directory\ndata_dir = '/kaggle/working/merged_images'\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',  # Automatically infer labels from directory structure\n    label_mode='categorical',  # Use categorical labels (one-hot encoding)\n    image_size=(224, 224),  # Resize images to 224x224\n    seed=50,  # Seed for random number generator\n    shuffle=True,  # Shuffle the dataset\n    batch_size=13  # Number of images to return in each batch\n)\n\n# Calculate the total number of samples\ntotal_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\n# Define sizes for training, validation, and test datasets\ntrain_size = int(0.8 * total_samples)  # 80% for training\nval_size = int(0.1 * total_samples)    # 10% for validation\ntest_size = total_samples - train_size - val_size  # Remaining 10% for testing\n\n# Create train, validation, and test datasets\ntrain_dataset = full_dataset.take(train_size)\nvalidation_dataset = full_dataset.skip(train_size).take(val_size)\ntest_dataset = full_dataset.skip(train_size + val_size).take(test_size)\n\n# Prefetch datasets for performance\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:10:38.975577Z","iopub.execute_input":"2025-02-24T21:10:38.975861Z","iopub.status.idle":"2025-02-24T21:10:39.397852Z","shell.execute_reply.started":"2025-02-24T21:10:38.975836Z","shell.execute_reply":"2025-02-24T21:10:39.396984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input\nfrom tensorflow.keras.metrics import Precision, Recall\n\ndef create_modelCNN(input_shape=(224, 224, 3)):\n    \"\"\"Create and compile a CNN model for binary classification.\"\"\"\n    # Initialize the Sequential model\n    model = Sequential()\n\n    # Add layers to the model\n    model.add(Input(shape=input_shape))  # Input layer for RGB images\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    # Flatten the output and add dense layers\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))  # Dropout layer for regularization\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))  # Another Dropout layer\n    model.add(Dense(2, activation='softmax'))  # Output layer for classification\n\n    # Compile the model\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:10:39.398678Z","iopub.execute_input":"2025-02-24T21:10:39.398951Z","iopub.status.idle":"2025-02-24T21:10:39.409832Z","shell.execute_reply.started":"2025-02-24T21:10:39.39893Z","shell.execute_reply":"2025-02-24T21:10:39.408869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Define the data directory\ndata_dir = '/kaggle/working/Hog_Images'\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',  # Automatically infer labels from directory structure\n    label_mode='categorical',  # Use categorical labels (one-hot encoding)\n    image_size=(224, 224),  # Resize images to 224x224\n    seed=50,  # Seed for random number generator\n    shuffle=True,  # Shuffle the dataset\n    batch_size=13  # Number of images to return in each batch\n)\n\n# Calculate the total number of samples\ntotal_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\n# Define sizes for training, validation, and test datasets\ntrain_size = int(0.8 * total_samples)  # 70% for training\nval_size = int(0.1 * total_samples)    # 10% for validation\ntest_size = total_samples - train_size - val_size  # Remaining 20% for testing\n\n# Create train, validation, and test datasets\ntrain_Neg_dataset = full_dataset.take(train_size)\nvalidation_Neg_dataset = full_dataset.skip(train_size).take(val_size)\ntest_Neg_dataset = full_dataset.skip(train_size + val_size).take(test_size)\n\n# Prefetch datasets for performance\ntrainHOG_original_dataset = train_Neg_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidationHOG_original_dataset = validation_Neg_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntestHOG_original_dataset = test_Neg_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# Print the number of samples in each dataset\nprint(f\"Train samples:      {train_size}     batches(13) ==> {train_size * 13}\")\nprint(f\"Validation samples: {val_size}       batches(13) ==> {val_size * 13}\")\nprint(f\"Test samples:       {test_size}       batches(13) ==> {test_size * 13}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:10:39.410883Z","iopub.execute_input":"2025-02-24T21:10:39.411159Z","iopub.status.idle":"2025-02-24T21:10:39.851624Z","shell.execute_reply.started":"2025-02-24T21:10:39.411132Z","shell.execute_reply":"2025-02-24T21:10:39.850982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_HOG = create_modelCNN()\n\nHog_Original = model_HOG.fit(\n           trainHOG_original_dataset,\n            validation_data= validationHOG_original_dataset, batch_size=13, \n            epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:10:39.852552Z","iopub.execute_input":"2025-02-24T21:10:39.852881Z","iopub.status.idle":"2025-02-24T21:13:28.54371Z","shell.execute_reply.started":"2025-02-24T21:10:39.852848Z","shell.execute_reply":"2025-02-24T21:13:28.542928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Hog_Original_Result = model_HOG.evaluate(testHOG_original_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:13:28.55085Z","iopub.execute_input":"2025-02-24T21:13:28.551074Z","iopub.status.idle":"2025-02-24T21:13:32.575968Z","shell.execute_reply.started":"2025-02-24T21:13:28.551054Z","shell.execute_reply":"2025-02-24T21:13:32.575266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the data directory\ndata_dir = '/kaggle/working/HogNeg_Images'\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',  # Automatically infer labels from directory structure\n    label_mode='categorical',  # Use categorical labels (one-hot encoding)\n    image_size=(224, 224),  # Resize images to 224x224\n    seed=50,  # Seed for random number generator\n    shuffle=True,  # Shuffle the dataset\n    batch_size=13  # Number of images to return in each batch\n)\n\n# Calculate the total number of samples\ntotal_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\n# Define sizes for training, validation, and test datasets\ntrain_size = int(0.8 * total_samples)  # 70% for training\nval_size = int(0.1 * total_samples)    # 10% for validation\ntest_size = total_samples - train_size - val_size  # Remaining 20% for testing\n\n# Create train, validation, and test datasets\ntrain_Neg_dataset = full_dataset.take(train_size)\nvalidation_Neg_dataset = full_dataset.skip(train_size).take(val_size)\ntest_Neg_dataset = full_dataset.skip(train_size + val_size).take(test_size)\n\n# Prefetch datasets for performance\ntrainHOG_Neg_dataset = train_Neg_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidationHOG_Neg_dataset = validation_Neg_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntestHOG_Neg_dataset = test_Neg_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# Print the number of samples in each dataset\nprint(f\"Train samples:      {train_size}     batches(13) ==> {train_size * 13}\")\nprint(f\"Validation samples: {val_size}       batches(13) ==> {val_size * 13}\")\nprint(f\"Test samples:       {test_size}       batches(13) ==> {test_size * 13}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:13:32.577229Z","iopub.execute_input":"2025-02-24T21:13:32.57748Z","iopub.status.idle":"2025-02-24T21:13:32.985073Z","shell.execute_reply.started":"2025-02-24T21:13:32.577452Z","shell.execute_reply":"2025-02-24T21:13:32.984402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_HOG_Neg = create_modelCNN()\n\nHog_Neg = model_HOG_Neg.fit(\n            trainHOG_Neg_dataset,\n            validation_data= validationHOG_Neg_dataset, batch_size=13, \n            epochs=10\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:13:32.98579Z","iopub.execute_input":"2025-02-24T21:13:32.986053Z","iopub.status.idle":"2025-02-24T21:16:15.816689Z","shell.execute_reply.started":"2025-02-24T21:13:32.986033Z","shell.execute_reply":"2025-02-24T21:16:15.815778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Hog_Neg_Result = model_HOG_Neg.evaluate(testHOG_Neg_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:16:15.817878Z","iopub.execute_input":"2025-02-24T21:16:15.818232Z","iopub.status.idle":"2025-02-24T21:16:19.636629Z","shell.execute_reply.started":"2025-02-24T21:16:15.8182Z","shell.execute_reply":"2025-02-24T21:16:19.63598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the data directory\ndata_dir = '/kaggle/working/HogAHE_Images'\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',  # Automatically infer labels from directory structure\n    label_mode='categorical',  # Use categorical labels (one-hot encoding)\n    image_size=(224, 224),  # Resize images to 224x224\n    seed=50,  # Seed for random number generator\n    shuffle=True,  # Shuffle the dataset\n    batch_size=13  # Number of images to return in each batch\n)\n\n# Calculate the total number of samples\ntotal_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\n# Define sizes for training, validation, and test datasets\ntrain_size = int(0.8 * total_samples)  # 70% for training\nval_size = int(0.1 * total_samples)    # 10% for validation\ntest_size = total_samples - train_size - val_size  # Remaining 20% for testing\n\n# Create train, validation, and test datasets\ntrain_AHE_dataset = full_dataset.take(train_size)\nvalidation_AHE_dataset = full_dataset.skip(train_size).take(val_size)\ntest_AHE_dataset = full_dataset.skip(train_size + val_size).take(test_size)\n\n# Prefetch datasets for performance\ntrainHOG_AHE_dataset = train_AHE_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidationHOG_AHE_dataset = validation_AHE_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntestHOG_AHE_dataset = test_AHE_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# Print the number of samples in each dataset\nprint(f\"Train samples:      {train_size}     batches(13) ==> {train_size * 13}\")\nprint(f\"Validation samples: {val_size}       batches(13) ==> {val_size * 13}\")\nprint(f\"Test samples:       {test_size}       batches(13) ==> {test_size * 13}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:16:19.637621Z","iopub.execute_input":"2025-02-24T21:16:19.637911Z","iopub.status.idle":"2025-02-24T21:16:20.073355Z","shell.execute_reply.started":"2025-02-24T21:16:19.637889Z","shell.execute_reply":"2025-02-24T21:16:20.072527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_HOG_AHE = create_modelCNN()\n\nHog_AHE = model_HOG_AHE.fit(\n            trainHOG_AHE_dataset,\n            validation_data= validationHOG_AHE_dataset,\n            batch_size= 13,\n            epochs=10\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:16:20.074214Z","iopub.execute_input":"2025-02-24T21:16:20.07452Z","iopub.status.idle":"2025-02-24T21:19:03.692459Z","shell.execute_reply.started":"2025-02-24T21:16:20.074489Z","shell.execute_reply":"2025-02-24T21:19:03.691687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Hog_AHE_Result = model_HOG_AHE.evaluate(testHOG_AHE_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:19:03.693365Z","iopub.execute_input":"2025-02-24T21:19:03.693702Z","iopub.status.idle":"2025-02-24T21:19:07.225622Z","shell.execute_reply.started":"2025-02-24T21:19:03.693677Z","shell.execute_reply":"2025-02-24T21:19:07.224968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot accuracy comparison\nplt.figure(figsize=(12, 8))\n\n# Accuracy\nplt.subplot(2, 2, 1)\nplt.plot(Hog_Original.history['accuracy'], label='HOG on Original Accuracy')\nplt.plot(Hog_Neg.history['accuracy'], label='HOG on Negative Accuracy')\nplt.plot(Hog_AHE.history['accuracy'], label='Hog on AHE Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\n# Validation Accuracy\nplt.subplot(2, 2, 2)\nplt.plot(Hog_Original.history['val_accuracy'], label='HOG on Original Val Accuracy')\nplt.plot(Hog_Neg.history['val_accuracy'], label='HOG on Negative Accuracy')\nplt.plot(Hog_AHE.history['val_accuracy'], label='Hog on AHE Accuracy')\nplt.title('Validation Accuracy')\nplt.legend()\n\n# Loss\nplt.subplot(2, 2, 3)\nplt.plot(Hog_Original.history['loss'], label='HOG on Original Loss')\nplt.plot(Hog_Neg.history['loss'], label='HOG on Negative Loss')\nplt.plot(Hog_AHE.history['loss'], label='Hog on AHE Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Validation Loss\nplt.subplot(2, 2, 4)\nplt.plot(Hog_Original.history['val_loss'], label='HOG on Original Val Loss')\nplt.plot(Hog_Neg.history['val_loss'], label='HOG on Negative Val Loss')\nplt.plot(Hog_AHE.history['val_loss'], label='Hog on AHE Val Loss')\nplt.title('Validation Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:19:07.226573Z","iopub.execute_input":"2025-02-24T21:19:07.226884Z","iopub.status.idle":"2025-02-24T21:19:08.003289Z","shell.execute_reply.started":"2025-02-24T21:19:07.226861Z","shell.execute_reply":"2025-02-24T21:19:08.002616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tabulate import tabulate\n\n# Sample results (replace these with your actual results)\naccuracy = [Hog_Original_Result[1], Hog_Neg_Result[1], Hog_AHE_Result[1]]\nprecision = [Hog_Original_Result[2], Hog_Neg_Result[2], Hog_AHE_Result[2]]\nrecall = [Hog_Original_Result[3], Hog_Neg_Result[3], Hog_AHE_Result[3]]\n\n# Create a DataFrame\ndata = {\n    \"Features\": [\"Histogram of Oriented Gradients (HOG)\", \"Histogram of Oriented Gradients (HOG)\", \"Histogram of Oriented Gradients (HOG)\"],\n    \"Image Enhancement\": [\"Original Image\", \"Negative Transformer\", \"Adaptive Histogram Equalization\"],\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall\": recall\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame in a beautiful format\nprint(\"\\n---------------------------------------------------------------------------------------------------------\\n\")\nprint(tabulate(df, headers='keys', showindex=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:19:08.004044Z","iopub.execute_input":"2025-02-24T21:19:08.004369Z","iopub.status.idle":"2025-02-24T21:19:08.030385Z","shell.execute_reply.started":"2025-02-24T21:19:08.004334Z","shell.execute_reply":"2025-02-24T21:19:08.029793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}